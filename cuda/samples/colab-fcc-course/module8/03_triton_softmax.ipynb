{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Softmax Implementation\n",
    "\n",
    "**FreeCodeCamp CUDA Course - Module 8: Triton**\n",
    "\n",
    "Original Course: [https://www.youtube.com/watch?v=86FAWCzIe_4](https://www.youtube.com/watch?v=86FAWCzIe_4)\n",
    "Source File: `02_softmax.py`\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Implement the softmax operation in Triton. Softmax is a fundamental operation in deep learning, used extensively in attention mechanisms and classification layers.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand the softmax operation and numerical stability\n",
    "2. Implement row-wise operations in Triton\n",
    "3. Use Triton's reduction operations (`tl.max`, `tl.sum`)\n",
    "4. Handle 2D tensor operations with proper stride calculations\n",
    "5. Compare Triton implementation with PyTorch's optimized softmax\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU and install Triton\n",
    "!nvidia-smi\n",
    "!pip install triton -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Softmax Operation\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "For an input vector $x = [x_1, x_2, ..., x_n]$:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "### Numerical Stability\n",
    "\n",
    "Direct computation can cause overflow with large values. Use the **max subtraction trick**:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i - \\max(x)}}{\\sum_{j=1}^{n} e^{x_j - \\max(x)}}$$\n",
    "\n",
    "### Properties\n",
    "- Output values are in range (0, 1)\n",
    "- Sum of outputs equals 1\n",
    "- Preserves ordering (larger inputs → larger outputs)\n",
    "\n",
    "---\n",
    "\n",
    "## Triton Softmax Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    output_ptr, \n",
    "    input_ptr, \n",
    "    input_row_stride, \n",
    "    output_row_stride, \n",
    "    n_cols,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute softmax along rows of a 2D tensor.\n",
    "    Each program processes one row.\n",
    "    \n",
    "    Args:\n",
    "        output_ptr: Pointer to output tensor\n",
    "        input_ptr: Pointer to input tensor\n",
    "        input_row_stride: Number of elements to skip between rows in input\n",
    "        output_row_stride: Number of elements to skip between rows in output\n",
    "        n_cols: Number of columns (elements per row)\n",
    "        BLOCK_SIZE: Block size (must be >= n_cols)\n",
    "    \"\"\"\n",
    "    # Each program handles one row\n",
    "    row_idx = tl.program_id(axis=0)\n",
    "\n",
    "    # Compute starting pointers for this row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    out_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "\n",
    "    # Create column offsets and mask\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    # Load the entire row into SRAM\n",
    "    # Use -inf for masked elements (won't affect max/exp)\n",
    "    row = tl.load(row_start_ptr + col_offsets, mask=mask, other=-float('inf'))\n",
    "\n",
    "    # Step 1: Find maximum value (for numerical stability)\n",
    "    row_max = tl.max(row, axis=0)\n",
    "    \n",
    "    # Step 2: Subtract max and exponentiate\n",
    "    numerator = tl.exp(row - row_max)\n",
    "    \n",
    "    # Step 3: Compute sum for normalization\n",
    "    denominator = tl.sum(numerator, axis=0)\n",
    "    \n",
    "    # Step 4: Normalize to get softmax\n",
    "    softmax_output = numerator / denominator\n",
    "    \n",
    "    # Store the result\n",
    "    tl.store(out_row_start_ptr + col_offsets, softmax_output, mask=mask)\n",
    "\n",
    "\n",
    "def triton_softmax(x):\n",
    "    \"\"\"Wrapper function to launch Triton softmax kernel.\"\"\"\n",
    "    n_rows, n_cols = x.shape\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    # Determine block size (round up to next power of 2, max 1024)\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    BLOCK_SIZE = min(BLOCK_SIZE, 1024)\n",
    "    \n",
    "    # Launch one program per row\n",
    "    grid = (n_rows,)\n",
    "    softmax_kernel[grid](\n",
    "        output, x,\n",
    "        x.stride(0), output.stride(0),\n",
    "        n_cols, \n",
    "        BLOCK_SIZE=BLOCK_SIZE\n",
    "    )\n",
    "    return output\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(256, 1024, device='cuda')\n",
    "\n",
    "# Compute softmax using PyTorch\n",
    "torch_result = torch.softmax(x, dim=1)\n",
    "\n",
    "# Compute softmax using Triton\n",
    "triton_result = triton_softmax(x)\n",
    "\n",
    "# Compare results\n",
    "max_diff = torch.max(torch.abs(torch_result - triton_result))\n",
    "print(f\"Maximum difference between PyTorch and Triton results: {max_diff:.2e}\")\n",
    "\n",
    "# Check if results are close\n",
    "is_close = torch.allclose(torch_result, triton_result, rtol=1e-5, atol=1e-5)\n",
    "print(f\"Results are close: {is_close}\")\n",
    "\n",
    "# Verify softmax properties\n",
    "print(f\"\\nSoftmax properties:\")\n",
    "print(f\"All values in (0,1): {torch.all((triton_result > 0) & (triton_result < 1))}\")\n",
    "print(f\"Row sums equal 1: {torch.allclose(triton_result.sum(dim=1), torch.ones(256, device='cuda'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding the Implementation\n",
    "\n",
    "### 1. Row-Wise Processing\n",
    "```python\n",
    "row_idx = tl.program_id(axis=0)  # Each program = one row\n",
    "```\n",
    "\n",
    "### 2. Stride Calculations\n",
    "For a 2D tensor with shape (256, 1024):\n",
    "- `input_row_stride = 1024` (elements between rows)\n",
    "- Row 0 starts at offset 0\n",
    "- Row 1 starts at offset 1024\n",
    "- Row i starts at offset `i * 1024`\n",
    "\n",
    "### 3. Memory Loading\n",
    "```python\n",
    "row = tl.load(row_start_ptr + col_offsets, mask=mask, other=-float('inf'))\n",
    "```\n",
    "- Loads entire row into SRAM (fast on-chip memory)\n",
    "- Masked elements set to -∞ (safe for max and exp)\n",
    "\n",
    "### 4. Reductions\n",
    "```python\n",
    "row_max = tl.max(row, axis=0)      # Find maximum\n",
    "denominator = tl.sum(numerator, axis=0)  # Sum for normalization\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_softmax(shape=(1024, 1024), num_runs=100):\n",
    "    \"\"\"Benchmark Triton vs PyTorch softmax.\"\"\"\n",
    "    x = torch.randn(shape, device='cuda')\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = torch.softmax(x, dim=1)\n",
    "        _ = triton_softmax(x)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark PyTorch\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = torch.softmax(x, dim=1)\n",
    "    torch.cuda.synchronize()\n",
    "    pytorch_time = (time.time() - start) / num_runs * 1000\n",
    "    \n",
    "    # Benchmark Triton\n",
    "    start = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        _ = triton_softmax(x)\n",
    "    torch.cuda.synchronize()\n",
    "    triton_time = (time.time() - start) / num_runs * 1000\n",
    "    \n",
    "    print(f\"Shape: {shape}\")\n",
    "    print(f\"PyTorch: {pytorch_time:.3f} ms\")\n",
    "    print(f\"Triton:  {triton_time:.3f} ms\")\n",
    "    print(f\"Speedup: {pytorch_time / triton_time:.2f}x\\n\")\n",
    "\n",
    "# Benchmark different shapes\n",
    "benchmark_softmax((256, 1024))\n",
    "benchmark_softmax((1024, 1024))\n",
    "benchmark_softmax((4096, 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Numerical Stability Demo\n",
    "\n",
    "Let's see why the max subtraction trick is important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor with large values that would overflow\n",
    "x_large = torch.tensor([[1000.0, 1001.0, 1002.0]], device='cuda')\n",
    "\n",
    "print(\"Testing numerical stability:\")\n",
    "print(f\"Input: {x_large}\")\n",
    "\n",
    "# Naive approach (would overflow)\n",
    "print(f\"\\nNaive exp (overflows): {torch.exp(x_large)}\")\n",
    "\n",
    "# With max subtraction (stable)\n",
    "x_shifted = x_large - x_large.max()\n",
    "print(f\"After max subtraction: {x_shifted}\")\n",
    "print(f\"Stable exp: {torch.exp(x_shifted)}\")\n",
    "\n",
    "# Our Triton implementation handles this\n",
    "result = triton_softmax(x_large)\n",
    "print(f\"\\nTriton softmax result: {result}\")\n",
    "print(f\"Sum: {result.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Column-wise Softmax**: Modify the kernel to compute softmax along columns (dim=0)\n",
    "   - Hint: Change how you organize programs and memory access\n",
    "\n",
    "2. **Temperature Scaling**: Add a temperature parameter: `softmax(x/T)`\n",
    "   - Higher temperature → more uniform distribution\n",
    "   - Lower temperature → more peaky distribution\n",
    "\n",
    "3. **Log-Softmax**: Implement log-softmax (more numerically stable)\n",
    "   - `log_softmax(x) = log(softmax(x)) = x - log(sum(exp(x)))`\n",
    "\n",
    "4. **Fused Softmax + Scale**: Compute `softmax(x) * scale` in one kernel\n",
    "   - Compare performance with separate operations\n",
    "\n",
    "5. **Different Shapes**: Test with various tensor shapes\n",
    "   - Very wide: (16, 8192)\n",
    "   - Very tall: (8192, 128)\n",
    "   - Square: (2048, 2048)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Numerical stability is crucial** - Always use max subtraction trick\n",
    "2. **Row-wise operations are natural in Triton** - One program per row\n",
    "3. **Reduction operations** (`tl.max`, `tl.sum`) are efficient\n",
    "4. **SRAM usage** - Loading entire rows enables fast reductions\n",
    "5. **Triton matches PyTorch performance** for well-optimized kernels\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **Module 9** to learn how to integrate custom CUDA operations with PyTorch!\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "*Use this space for your learning notes:*\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
