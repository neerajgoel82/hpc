{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch CUDA Extensions - Custom Polynomial Activation\n",
    "\n",
    "**FreeCodeCamp CUDA Course - Module 9: PyTorch Extensions**\n",
    "\n",
    "Original Course: [https://www.youtube.com/watch?v=86FAWCzIe_4](https://www.youtube.com/watch?v=86FAWCzIe_4)\n",
    "Source Files: `polynomial_cuda.cu`, `polynomial_activation.py`\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Learn how to integrate custom CUDA kernels with PyTorch. This enables you to write high-performance operations while maintaining PyTorch's automatic differentiation and ease of use.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand PyTorch's C++/CUDA extension mechanism\n",
    "2. Write CUDA kernels that integrate with PyTorch tensors\n",
    "3. Use PyBind11 to expose C++/CUDA functions to Python\n",
    "4. Compare custom CUDA operations with PyTorch built-ins\n",
    "5. Understand when to write custom CUDA extensions\n",
    "\n",
    "---\n",
    "\n",
    "## Why Custom CUDA Extensions?\n",
    "\n",
    "### Use Cases\n",
    "1. **Novel Operations**: Operations not available in PyTorch\n",
    "2. **Fused Kernels**: Combine multiple operations for efficiency\n",
    "3. **Specialized Algorithms**: Domain-specific optimizations\n",
    "4. **Research**: Implementing cutting-edge algorithms\n",
    "\n",
    "### Trade-offs\n",
    "- ‚úÖ Maximum performance control\n",
    "- ‚úÖ Can implement any GPU algorithm\n",
    "- ‚ùå More complex than pure PyTorch\n",
    "- ‚ùå Compilation required\n",
    "- ‚ùå Platform-specific code\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required tools\n",
    "!pip install torch ninja -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example: Polynomial Activation Function\n",
    "\n",
    "We'll implement a custom activation: $f(x) = x^2 + x + 1$\n",
    "\n",
    "### CUDA Kernel Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile polynomial_cuda_kernel.cu\n",
    "#include <torch/extension.h>\n",
    "#include <cuda.h>\n",
    "#include <cuda_runtime.h>\n",
    "\n",
    "// CUDA kernel for polynomial activation: f(x) = x^2 + x + 1\n",
    "template <typename scalar_t>\n",
    "__global__ void polynomial_activation_cuda_kernel(\n",
    "    const scalar_t* __restrict__ input,\n",
    "    scalar_t* __restrict__ output,\n",
    "    size_t size) {\n",
    "    \n",
    "    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (idx < size) {\n",
    "        const scalar_t x = input[idx];\n",
    "        output[idx] = x * x + x + 1.0;\n",
    "    }\n",
    "}\n",
    "\n",
    "// C++ wrapper that dispatches to CUDA kernel\n",
    "torch::Tensor polynomial_activation_cuda(torch::Tensor input) {\n",
    "    // Create output tensor with same shape and type as input\n",
    "    auto output = torch::zeros_like(input);\n",
    "    \n",
    "    const int threads = 256;\n",
    "    const int blocks = (input.numel() + threads - 1) / threads;\n",
    "    \n",
    "    // Launch kernel with proper type dispatching\n",
    "    AT_DISPATCH_FLOATING_TYPES(input.type(), \"polynomial_activation_cuda\", ([&] {\n",
    "        polynomial_activation_cuda_kernel<scalar_t><<<blocks, threads>>>(\n",
    "            input.data_ptr<scalar_t>(),\n",
    "            output.data_ptr<scalar_t>(),\n",
    "            input.numel()\n",
    "        );\n",
    "    }));\n",
    "    \n",
    "    return output;\n",
    "}\n",
    "\n",
    "// PyBind11 bindings to expose to Python\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"polynomial_activation\", &polynomial_activation_cuda, \n",
    "          \"Polynomial activation function (CUDA)\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Extension\n",
    "\n",
    "PyTorch provides utilities to compile CUDA extensions on-the-fly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "# Compile the CUDA extension\n",
    "# This may take a minute the first time\n",
    "polynomial_cuda = load(\n",
    "    name='polynomial_cuda',\n",
    "    sources=['polynomial_cuda_kernel.cu'],\n",
    "    extra_cuda_cflags=['-O2'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úì CUDA extension compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PyTorch Integration\n",
    "\n",
    "Now let's create a PyTorch module that uses our custom CUDA kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "class CUDAPolynomialActivation(torch.autograd.Function):\n",
    "    \"\"\"PyTorch autograd Function wrapper for CUDA kernel.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        return polynomial_cuda.polynomial_activation(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # For a complete implementation, you'd implement the derivative:\n",
    "        # df/dx = 2x + 1\n",
    "        # For this demo, we'll leave it unimplemented\n",
    "        raise NotImplementedError(\"Backward pass not implemented\")\n",
    "\n",
    "\n",
    "class PolynomialActivation(nn.Module):\n",
    "    \"\"\"PyTorch Module supporting both PyTorch and CUDA implementations.\"\"\"\n",
    "    \n",
    "    def __init__(self, implementation='pytorch'):\n",
    "        super().__init__()\n",
    "        self.implementation = implementation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.implementation == 'pytorch':\n",
    "            # Pure PyTorch implementation\n",
    "            return x**2 + x + 1\n",
    "        elif self.implementation == 'cuda':\n",
    "            # Custom CUDA implementation\n",
    "            return CUDAPolynomialActivation.apply(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown implementation: {self.implementation}\")\n",
    "\n",
    "\n",
    "# Test correctness\n",
    "torch.manual_seed(0)\n",
    "x = torch.randn(1000, device='cuda')\n",
    "\n",
    "pytorch_act = PolynomialActivation(implementation='pytorch').cuda()\n",
    "cuda_act = PolynomialActivation(implementation='cuda').cuda()\n",
    "\n",
    "pytorch_out = pytorch_act(x)\n",
    "cuda_out = cuda_act(x)\n",
    "\n",
    "# Verify results match\n",
    "print(\"Correctness Check:\")\n",
    "print(f\"Max difference: {torch.max(torch.abs(pytorch_out - cuda_out))}\")\n",
    "print(f\"Results match: {torch.allclose(pytorch_out, cuda_out)}\")\n",
    "\n",
    "# Show sample outputs\n",
    "print(f\"\\nSample input: {x[:5]}\")\n",
    "print(f\"PyTorch output: {pytorch_out[:5]}\")\n",
    "print(f\"CUDA output: {cuda_out[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(func, x, name, num_runs=1000):\n",
    "    \"\"\"Benchmark a function.\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        func(x)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        func(x)\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs * 1000\n",
    "    return f\"{name}: {avg_time:.4f} ms\"\n",
    "\n",
    "\n",
    "# Benchmark with different sizes\n",
    "for size in [10_000, 100_000, 1_000_000, 10_000_000]:\n",
    "    print(f\"\\nBenchmark with {size:,} elements:\")\n",
    "    x = torch.randn(size, device='cuda')\n",
    "    \n",
    "    pytorch_time = benchmark(pytorch_act, x, \"PyTorch\")\n",
    "    cuda_time = benchmark(cuda_act, x, \"CUDA   \")\n",
    "    \n",
    "    print(pytorch_time)\n",
    "    print(cuda_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding the Code\n",
    "\n",
    "### 1. CUDA Kernel Template\n",
    "```cpp\n",
    "template <typename scalar_t>\n",
    "__global__ void polynomial_activation_cuda_kernel(...) {\n",
    "```\n",
    "- Template allows support for `float`, `double`, etc.\n",
    "- PyTorch dispatches to correct type at runtime\n",
    "\n",
    "### 2. `__restrict__` Keyword\n",
    "```cpp\n",
    "const scalar_t* __restrict__ input\n",
    "```\n",
    "- Tells compiler: pointers don't alias (don't overlap)\n",
    "- Enables more aggressive optimizations\n",
    "\n",
    "### 3. Type Dispatching\n",
    "```cpp\n",
    "AT_DISPATCH_FLOATING_TYPES(input.type(), \"name\", ([&] {\n",
    "    // Code that uses scalar_t\n",
    "}));\n",
    "```\n",
    "- PyTorch macro that generates code for each supported type\n",
    "- Ensures type safety\n",
    "\n",
    "### 4. PyBind11 Binding\n",
    "```cpp\n",
    "PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n",
    "    m.def(\"polynomial_activation\", &polynomial_activation_cuda, \"...\");\n",
    "}\n",
    "```\n",
    "- Exposes C++ function to Python\n",
    "- Automatically handles type conversion\n",
    "\n",
    "---\n",
    "\n",
    "## When Should You Write CUDA Extensions?\n",
    "\n",
    "### ‚úÖ Good Use Cases\n",
    "- Custom operations not in PyTorch\n",
    "- Fusing multiple operations to reduce memory bandwidth\n",
    "- Specialized algorithms (e.g., sparse operations, custom attention)\n",
    "- Research implementations\n",
    "\n",
    "### ‚ùå Avoid If\n",
    "- Operation already exists in PyTorch (it's probably faster)\n",
    "- Can be implemented efficiently with existing PyTorch ops\n",
    "- Development time > performance gain\n",
    "- Need cross-platform support\n",
    "\n",
    "### üí° Alternative: Triton\n",
    "- Easier to write than CUDA\n",
    "- Still high performance\n",
    "- Better for most use cases\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Implement Backward Pass**: Add gradient computation\n",
    "   - Derivative: $\\frac{df}{dx} = 2x + 1$\n",
    "   - Test with `torch.autograd.gradcheck`\n",
    "\n",
    "2. **Different Polynomial**: Implement $f(x) = x^3 - 2x^2 + x$\n",
    "\n",
    "3. **Fused Operation**: Combine polynomial activation with another operation\n",
    "   - Example: $f(x) = \\text{ReLU}(x^2 + x + 1)$\n",
    "   - Compare with unfused version\n",
    "\n",
    "4. **Element-wise Binary Operation**: Implement $(x + y)^2$\n",
    "   - Takes two inputs\n",
    "   - Compare with PyTorch: `(x + y) ** 2`\n",
    "\n",
    "5. **Profiling**: Use Nsight Compute to analyze your kernel\n",
    "   - Check memory bandwidth utilization\n",
    "   - Look for optimization opportunities\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **PyTorch extensions bridge Python and CUDA** seamlessly\n",
    "2. **Template programming** enables type-generic kernels\n",
    "3. **Just-in-time compilation** makes development faster\n",
    "4. **Custom extensions are powerful** but add complexity\n",
    "5. **Consider Triton first** for new implementations\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [PyTorch Custom C++ and CUDA Extensions](https://pytorch.org/tutorials/advanced/cpp_extension.html)\n",
    "- [PyTorch C++ API Documentation](https://pytorch.org/cppdocs/)\n",
    "- [Examples: PyTorch Extension Examples](https://github.com/pytorch/extension-cpp)\n",
    "\n",
    "---\n",
    "\n",
    "## Course Complete!\n",
    "\n",
    "Congratulations on completing the FreeCodeCamp CUDA Course notebooks! You now have:\n",
    "\n",
    "- ‚úÖ CUDA programming fundamentals\n",
    "- ‚úÖ Memory optimization techniques  \n",
    "- ‚úÖ Experience with CUDA libraries (cuBLAS, cuDNN)\n",
    "- ‚úÖ Knowledge of Triton for high-level GPU programming\n",
    "- ‚úÖ Skills to create PyTorch CUDA extensions\n",
    "\n",
    "**Next Steps:**\n",
    "- Build your own CUDA projects\n",
    "- Optimize existing deep learning code\n",
    "- Explore advanced topics (multi-GPU, tensor cores)\n",
    "- Join CUDA/GPU programming communities\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "*Use this space for your learning notes:*\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
