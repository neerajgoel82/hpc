{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triton Vector Addition with Performance Benchmarking\n",
    "\n",
    "**FreeCodeCamp CUDA Course - Module 8: Triton**\n",
    "\n",
    "Original Course: [https://www.youtube.com/watch?v=86FAWCzIe_4](https://www.youtube.com/watch?v=86FAWCzIe_4)\n",
    "Source File: `01_vec_add.py`\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "Introduction to OpenAI's Triton language for GPU programming. Learn how Triton provides a higher-level abstraction than CUDA while maintaining performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand Triton's programming model (blocked programs vs CUDA's scalar programs)\n",
    "2. Write a Triton kernel for vector addition\n",
    "3. Compare Triton performance with PyTorch native operations\n",
    "4. Use Triton's benchmarking utilities\n",
    "5. Understand memory masking and pointer arithmetic in Triton\n",
    "\n",
    "---\n",
    "\n",
    "## Setup: Install Triton\n",
    "\n",
    "First, ensure GPU is enabled and install Triton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Triton\n",
    "!pip install triton -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Triton vs CUDA: Key Differences\n",
    "\n",
    "### CUDA Programming Model\n",
    "- **Scalar program, blocked threads**: Write code for one thread, launch many\n",
    "- Manual memory management\n",
    "- Explicit synchronization (`__syncthreads()`)\n",
    "\n",
    "### Triton Programming Model\n",
    "- **Blocked program, scalar threads**: Write code for a block of elements\n",
    "- Automatic memory coalescing and caching\n",
    "- Higher-level abstractions\n",
    "- Similar performance to hand-tuned CUDA\n",
    "\n",
    "---\n",
    "\n",
    "## Triton Vector Addition Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(x_ptr,  # Pointer to first input vector\n",
    "               y_ptr,  # Pointer to second input vector\n",
    "               output_ptr,  # Pointer to output vector\n",
    "               n_elements,  # Size of the vector\n",
    "               BLOCK_SIZE: tl.constexpr,  # Number of elements each program processes\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Triton kernel for element-wise vector addition.\n",
    "    \n",
    "    Equivalent CUDA:\n",
    "    __global__ void add_kernel(float* x, float* y, float* output, int n) {\n",
    "        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "        if (idx < n) {\n",
    "            output[idx] = x[idx] + y[idx];\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    # Get program ID (similar to blockIdx.x in CUDA)\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # Calculate which elements this program will process\n",
    "    # For 256-element vector with BLOCK_SIZE=64:\n",
    "    # Program 0: elements [0:64]\n",
    "    # Program 1: elements [64:128]\n",
    "    # Program 2: elements [128:192]\n",
    "    # Program 3: elements [192:256]\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "\n",
    "    # Create mask for bounds checking (handles non-multiple of BLOCK_SIZE)\n",
    "    mask = offsets < n_elements\n",
    "\n",
    "    # Load x and y from DRAM with masking\n",
    "    x = tl.load(x_ptr + offsets, mask=mask)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask)\n",
    "    \n",
    "    # Perform addition\n",
    "    output = x + y\n",
    "    \n",
    "    # Store result back to DRAM\n",
    "    tl.store(output_ptr + offsets, output, mask=mask)\n",
    "\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor):\n",
    "    \"\"\"Wrapper function to launch Triton kernel.\"\"\"\n",
    "    # Preallocate output\n",
    "    output = torch.empty_like(x)\n",
    "    assert x.is_cuda and y.is_cuda and output.is_cuda\n",
    "    n_elements = output.numel()\n",
    "    \n",
    "    # Define grid (number of program instances to launch)\n",
    "    # Similar to: num_blocks = (n_elements + BLOCK_SIZE - 1) // BLOCK_SIZE\n",
    "    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )\n",
    "    \n",
    "    # Launch kernel\n",
    "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# Test the kernel\n",
    "torch.manual_seed(0)\n",
    "size = 2**20  # 1M elements\n",
    "x = torch.rand(size, device='cuda')\n",
    "y = torch.rand(size, device='cuda')\n",
    "\n",
    "# Triton result\n",
    "triton_result = add(x, y)\n",
    "\n",
    "# PyTorch result\n",
    "torch_result = x + y\n",
    "\n",
    "# Verify correctness\n",
    "print(f\"Max difference: {torch.max(torch.abs(triton_result - torch_result))}\")\n",
    "print(f\"Results match: {torch.allclose(triton_result, torch_result)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Performance Benchmarking\n",
    "\n",
    "Now let's benchmark Triton against PyTorch's optimized operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['size'],  # X-axis: vector size\n",
    "        x_vals=[2**i for i in range(12, 28, 1)],  # 4K to 128M elements\n",
    "        x_log=True,  # Logarithmic x-axis\n",
    "        line_arg='provider',  # Compare different implementations\n",
    "        line_vals=['triton', 'torch'],\n",
    "        line_names=['Triton', 'PyTorch'],\n",
    "        styles=[('blue', '-'), ('green', '-')],\n",
    "        ylabel='GB/s',  # Memory bandwidth achieved\n",
    "        plot_name='vector-add-performance',\n",
    "        args={},\n",
    "    ))\n",
    "def benchmark(size, provider):\n",
    "    x = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    y = torch.rand(size, device='cuda', dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: x + y, quantiles=quantiles)\n",
    "    elif provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: add(x, y), quantiles=quantiles)\n",
    "    \n",
    "    # Calculate bandwidth: 3 arrays (2 reads, 1 write) * size * bytes per element / time\n",
    "    gbps = lambda ms: 3 * x.numel() * x.element_size() / ms * 1e-6\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Concepts Explained\n",
    "\n",
    "### 1. Program ID (`tl.program_id`)\n",
    "- Similar to `blockIdx.x` in CUDA\n",
    "- Identifies which \"block\" of work this program handles\n",
    "\n",
    "### 2. Memory Masking\n",
    "```python\n",
    "mask = offsets < n_elements\n",
    "```\n",
    "- Prevents out-of-bounds memory access\n",
    "- Handles cases where vector size isn't a multiple of BLOCK_SIZE\n",
    "\n",
    "### 3. Blocked Programming\n",
    "- Each program processes multiple elements (BLOCK_SIZE)\n",
    "- Triton automatically optimizes memory access patterns\n",
    "\n",
    "### 4. Memory Bandwidth\n",
    "- Vector add is memory-bound (limited by DRAM bandwidth)\n",
    "- Formula: `(2 reads + 1 write) * size * 4 bytes / time`\n",
    "- T4 GPU peak: ~320 GB/s\n",
    "\n",
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Different Block Sizes**: Modify `BLOCK_SIZE` to 256, 512, 2048\n",
    "   - How does it affect performance?\n",
    "   - What's the optimal block size?\n",
    "\n",
    "2. **Vector Multiplication**: Create a `multiply_kernel` for element-wise multiplication\n",
    "\n",
    "3. **Fused Operations**: Implement `output = (x + y) * 2` in a single kernel\n",
    "   - Compare with two separate operations\n",
    "\n",
    "4. **Multiple Vectors**: Extend to `output = x + y + z`\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Triton provides CUDA-like performance with Python simplicity**\n",
    "2. **Memory-bound operations achieve similar performance across implementations**\n",
    "3. **Triton automatically handles memory coalescing and optimization**\n",
    "4. **Block-based programming model is more intuitive for many algorithms**\n",
    "5. **Built-in benchmarking tools make performance analysis easy**\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to **02_triton_softmax.ipynb** to learn more advanced Triton programming with the softmax operation!\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "*Use this space for your learning notes:*\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
