{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 47: Neural Network from Scratch",
    "## Phase 8: Real-World Applications",
    "",
    "**Learning Objectives:**",
    "- Understand neural network",
    "- Learn backpropagation",
    "- Master deep learning",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Neural Network from Scratch",
    "",
    "**Topics Covered:**",
    "- neural network",
    "- backpropagation",
    "- deep learning",
    "",
    "**Key Concepts:**",
    "This notebook covers neural network in the context of Phase 8: Real-World Applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Neural Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n#include <math.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__device__ float sigmoid(float x) {\n    return 1.0f / (1.0f + expf(-x));\n}\n\n__device__ float sigmoid_derivative(float x) {\n    float s = sigmoid(x);\n    return s * (1.0f - s);\n}\n\n__global__ void forwardLayer(float *input, float *weights, float *bias,\n                              float *output, int inputSize, int outputSize) {\n    int neuron = blockIdx.x * blockDim.x + threadIdx.x;\n    if (neuron >= outputSize) return;\n\n    float sum = bias[neuron];\n    for (int i = 0; i < inputSize; i++) {\n        sum += input[i] * weights[neuron * inputSize + i];\n    }\n    output[neuron] = sigmoid(sum);\n}\n\n__global__ void backwardLayer(float *input, float *weights, float *delta,\n                               float *prevDelta, int inputSize, int outputSize) {\n    int neuron = blockIdx.x * blockDim.x + threadIdx.x;\n    if (neuron >= inputSize) return;\n\n    float error = 0.0f;\n    for (int i = 0; i < outputSize; i++) {\n        error += delta[i] * weights[i * inputSize + neuron];\n    }\n    prevDelta[neuron] = error * sigmoid_derivative(input[neuron]);\n}\n\n__global__ void updateWeights(float *weights, float *input, float *delta,\n                               int inputSize, int outputSize, float learningRate) {\n    int neuron = blockIdx.x * blockDim.x + threadIdx.x;\n    if (neuron >= outputSize) return;\n\n    for (int i = 0; i < inputSize; i++) {\n        weights[neuron * inputSize + i] += learningRate * delta[neuron] * input[i];\n    }\n}\n\nint main() {\n    printf(\"=== Neural Network: Forward & Backward Pass ===\\n\\n\");\n\n    const int inputSize = 784;    // 28x28 image\n    const int hiddenSize = 128;\n    const int outputSize = 10;    // 10 digits\n    const float learningRate = 0.01f;\n\n    // Allocate device memory\n    float *d_input, *d_hidden, *d_output;\n    float *d_w1, *d_w2, *d_b1, *d_b2;\n    float *d_delta_hidden, *d_delta_output;\n\n    CUDA_CHECK(cudaMalloc(&d_input, inputSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_hidden, hiddenSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_output, outputSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_w1, inputSize * hiddenSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_w2, hiddenSize * outputSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_b1, hiddenSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_b2, outputSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_delta_hidden, hiddenSize * sizeof(float)));\n    CUDA_CHECK(cudaMalloc(&d_delta_output, outputSize * sizeof(float)));\n\n    // Initialize with random weights (simplified - would use cuRAND in production)\n    float *h_w1 = (float*)malloc(inputSize * hiddenSize * sizeof(float));\n    float *h_w2 = (float*)malloc(hiddenSize * outputSize * sizeof(float));\n    for (int i = 0; i < inputSize * hiddenSize; i++) {\n        h_w1[i] = (rand() / (float)RAND_MAX - 0.5f) * 0.1f;\n    }\n    for (int i = 0; i < hiddenSize * outputSize; i++) {\n        h_w2[i] = (rand() / (float)RAND_MAX - 0.5f) * 0.1f;\n    }\n\n    CUDA_CHECK(cudaMemcpy(d_w1, h_w1, inputSize * hiddenSize * sizeof(float),\n                          cudaMemcpyHostToDevice));\n    CUDA_CHECK(cudaMemcpy(d_w2, h_w2, hiddenSize * outputSize * sizeof(float),\n                          cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n\n    // Forward pass\n    cudaEventRecord(start);\n    forwardLayer<<<(hiddenSize + threads - 1) / threads, threads>>>\n        (d_input, d_w1, d_b1, d_hidden, inputSize, hiddenSize);\n    forwardLayer<<<(outputSize + threads - 1) / threads, threads>>>\n        (d_hidden, d_w2, d_b2, d_output, hiddenSize, outputSize);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float forwardTime;\n    cudaEventElapsedTime(&forwardTime, start, stop);\n\n    // Backward pass\n    cudaEventRecord(start);\n    backwardLayer<<<(hiddenSize + threads - 1) / threads, threads>>>\n        (d_hidden, d_w2, d_delta_output, d_delta_hidden, hiddenSize, outputSize);\n    updateWeights<<<(hiddenSize + threads - 1) / threads, threads>>>\n        (d_w1, d_input, d_delta_hidden, inputSize, hiddenSize, learningRate);\n    updateWeights<<<(outputSize + threads - 1) / threads, threads>>>\n        (d_w2, d_hidden, d_delta_output, hiddenSize, outputSize, learningRate);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float backwardTime;\n    cudaEventElapsedTime(&backwardTime, start, stop);\n\n    printf(\"Network architecture: %d -> %d -> %d\\n\",\n           inputSize, hiddenSize, outputSize);\n    printf(\"Forward pass:  %.3f ms\\n\", forwardTime);\n    printf(\"Backward pass: %.3f ms\\n\", backwardTime);\n    printf(\"Total:         %.3f ms\\n\", forwardTime + backwardTime);\n    printf(\"\\nParameters: %d weights\\n\",\n           inputSize * hiddenSize + hiddenSize * outputSize);\n\n    free(h_w1);\n    free(h_w2);\n    cudaFree(d_input);\n    cudaFree(d_hidden);\n    cudaFree(d_output);\n    cudaFree(d_w1);\n    cudaFree(d_w2);\n    cudaFree(d_b1);\n    cudaFree(d_b2);\n    cudaFree(d_delta_hidden);\n    cudaFree(d_delta_output);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Neural Network ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Neural networks are matrix operations\n2. Forward pass: layer-by-layer computation\n3. Activation functions on GPU\n4. cuDNN provides optimized primitives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **48_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}