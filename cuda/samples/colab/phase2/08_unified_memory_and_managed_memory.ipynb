{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 08: Unified Memory and Managed Memory",
    "## Phase 2: Memory Management",
    "",
    "**Learning Objectives:**",
    "- Understand Unified Memory concept",
    "- Use cudaMallocManaged for simpler code",
    "- Learn about page faults and migration",
    "- Understand performance implications",
    "- Use prefetching for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Unified Memory",
    "",
    "**Unified Memory:**",
    "- Single pointer accessible from CPU and GPU",
    "- Automatic data migration",
    "- Simpler code, but requires understanding",
    "",
    "**Functions:**",
    "```cuda",
    "cudaMallocManaged(&ptr, size);      // Allocate managed memory",
    "cudaMemPrefetchAsync(ptr, size, device);  // Prefetch data",
    "cudaMemAdvise(ptr, size, advice, device); // Give hints",
    "```",
    "",
    "**Benefits:**",
    "- Simplified memory management",
    "- Automatic migration",
    "- Oversubscription support",
    "",
    "**Considerations:**",
    "- Page fault overhead",
    "- Migration costs",
    "- Requires compute capability 6.0+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Unified Memory and Managed Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    printf(\"=== Unified Memory Basic Example ===\\n\\n\");\n\n    // Allocate unified memory (accessible from both CPU and GPU)\n    float *a, *b, *c;\n    CUDA_CHECK(cudaMallocManaged(&a, size));\n    CUDA_CHECK(cudaMallocManaged(&b, size));\n    CUDA_CHECK(cudaMallocManaged(&c, size));\n\n    printf(\"\u2713 Allocated unified memory: %zu MB\\n\", size * 3 / (1024*1024));\n\n    // Initialize on CPU (no explicit transfer needed!)\n    for (int i = 0; i < n; i++) {\n        a[i] = i * 1.0f;\n        b[i] = i * 2.0f;\n    }\n    printf(\"\u2713 Initialized data on CPU\\n\");\n\n    // Launch kernel (data automatically migrated to GPU)\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n\n    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(a, b, c, n);\n    CUDA_CHECK(cudaDeviceSynchronize());\n    printf(\"\u2713 Kernel executed on GPU\\n\");\n\n    // Access result on CPU (automatically migrated back)\n    bool correct = true;\n    for (int i = 0; i < n; i++) {\n        if (c[i] != a[i] + b[i]) {\n            correct = false;\n            break;\n        }\n    }\n    printf(\"\u2713 Verified results on CPU\\n\");\n    printf(\"\\nResult: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n    printf(\"Sample: %.1f + %.1f = %.1f\\n\", a[0], b[0], c[0]);\n\n    // Single free call for unified memory\n    cudaFree(a);\n    cudaFree(b);\n    cudaFree(c);\n\n    printf(\"\\nAdvantages: No explicit cudaMemcpy calls!\\n\");\n    printf(\"Disadvantage: Implicit transfers may be slower\\n\");\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) c[idx] = a[idx] + b[idx];\n}\n\nint main() {\n    printf(\"=== Unified Memory ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    // Allocate unified memory\n    float *a, *b, *c;\n    CUDA_CHECK(cudaMallocManaged(&a, size));\n    CUDA_CHECK(cudaMallocManaged(&b, size));\n    CUDA_CHECK(cudaMallocManaged(&c, size));\n\n    // Initialize on CPU\n    for (int i = 0; i < n; i++) {\n        a[i] = i * 1.0f;\n        b[i] = i * 2.0f;\n    }\n\n    // Launch kernel\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n    vectorAdd<<<blocks, threads>>>(a, b, c, n);\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    // Verify on CPU\n    bool correct = true;\n    for (int i = 0; i < n; i++) {\n        if (c[i] != a[i] + b[i]) {\n            correct = false;\n            break;\n        }\n    }\n\n    printf(\"Result: %s\\n\", correct ? \"PASSED\" : \"FAILED\");\n    printf(\"Unified memory simplifies programming!\\n\");\n\n    cudaFree(a); cudaFree(b); cudaFree(c);\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Unified Memory simplifies memory management\n2. cudaMallocManaged for automatic migration\n3. Page faulting and data migration\n4. Prefetching with cudaMemPrefetchAsync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **09_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}