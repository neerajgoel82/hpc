{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 09: Shared Memory Basics",
    "## Phase 2: Memory Management - Shared Memory",
    "",
    "**Learning Objectives:**",
    "- Understand shared memory architecture",
    "- Declare and use shared memory",
    "- Implement basic tiling with shared memory",
    "- Learn synchronization with __syncthreads()",
    "- Understand shared memory scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Shared Memory",
    "",
    "**What is Shared Memory?**",
    "- Fast on-chip memory (similar to L1 cache)",
    "- Shared among threads in a block",
    "- ~100x faster than global memory",
    "- Limited size (48-96 KB per SM)",
    "",
    "**Declaration:**",
    "```cuda",
    "__shared__ float sharedData[256];  // Static allocation",
    "extern __shared__ float sharedData[];  // Dynamic allocation",
    "```",
    "",
    "**Key Points:**",
    "- Requires __syncthreads() for synchronization",
    "- Per-block scope",
    "- Bank conflicts can reduce performance",
    "- Ideal for data reuse within block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Shared Memory Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n#define TILE_SIZE 256\n\n__global__ void arrayReverseShared(float *input, float *output, int n) {\n    __shared__ float tile[TILE_SIZE];\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Load data into shared memory\n    if (idx < n) {\n        tile[threadIdx.x] = input[idx];\n    }\n    __syncthreads();  // Wait for all threads to load\n\n    // Write in reverse order\n    if (idx < n) {\n        output[idx] = tile[blockDim.x - 1 - threadIdx.x];\n    }\n}\n\n__global__ void arrayReverseGlobal(float *input, float *output, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < n) {\n        int reverseIdx = (blockIdx.x + 1) * blockDim.x - 1 - threadIdx.x;\n        if (reverseIdx < n) {\n            output[idx] = input[reverseIdx];\n        }\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    printf(\"=== Shared Memory Demonstration ===\\n\\n\");\n\n    float *h_input = (float*)malloc(size);\n    float *h_output = (float*)malloc(size);\n\n    for (int i = 0; i < n; i++) h_input[i] = i;\n\n    float *d_input, *d_output;\n    cudaMalloc(&d_input, size);\n    cudaMalloc(&d_output, size);\n    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threadsPerBlock = TILE_SIZE;\n    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n\n    // Test with shared memory\n    cudaEventRecord(start);\n    arrayReverseShared<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float time_shared;\n    cudaEventElapsedTime(&time_shared, start, stop);\n\n    // Test without shared memory\n    cudaEventRecord(start);\n    arrayReverseGlobal<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float time_global;\n    cudaEventElapsedTime(&time_global, start, stop);\n\n    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n\n    printf(\"Shared memory: %.3f ms\\n\", time_shared);\n    printf(\"Global memory: %.3f ms\\n\", time_global);\n    printf(\"Speedup: %.2fx\\n\\n\", time_global / time_shared);\n\n    printf(\"Shared memory is faster due to on-chip access!\\n\");\n\n    free(h_input); free(h_output);\n    cudaFree(d_input); cudaFree(d_output);\n    cudaEventDestroy(start); cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n#define TILE_SIZE 256\n\n__global__ void arrayReverseShared(float *input, float *output, int n) {\n    __shared__ float tile[TILE_SIZE];\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Load data into shared memory\n    if (idx < n) {\n        tile[threadIdx.x] = input[idx];\n    }\n    __syncthreads();  // Wait for all threads to load\n\n    // Write in reverse order\n    if (idx < n) {\n        output[idx] = tile[blockDim.x - 1 - threadIdx.x];\n    }\n}\n\n__global__ void arrayReverseGlobal(float *input, float *output, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < n) {\n        int reverseIdx = (blockIdx.x + 1) * blockDim.x - 1 - threadIdx.x;\n        if (reverseIdx < n) {\n            output[idx] = input[reverseIdx];\n        }\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    printf(\"=== Shared Memory Demonstration ===\\n\\n\");\n\n    float *h_input = (float*)malloc(size);\n    float *h_output = (float*)malloc(size);\n\n    for (int i = 0; i < n; i++) h_input[i] = i;\n\n    float *d_input, *d_output;\n    cudaMalloc(&d_input, size);\n    cudaMalloc(&d_output, size);\n    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threadsPerBlock = TILE_SIZE;\n    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n\n    // Test with shared memory\n    cudaEventRecord(start);\n    arrayReverseShared<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float time_shared;\n    cudaEventElapsedTime(&time_shared, start, stop);\n\n    // Test without shared memory\n    cudaEventRecord(start);\n    arrayReverseGlobal<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float time_global;\n    cudaEventElapsedTime(&time_global, start, stop);\n\n    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n\n    printf(\"Shared memory: %.3f ms\\n\", time_shared);\n    printf(\"Global memory: %.3f ms\\n\", time_global);\n    printf(\"Speedup: %.2fx\\n\\n\", time_global / time_shared);\n\n    printf(\"Shared memory is faster due to on-chip access!\\n\");\n\n    free(h_input); free(h_output);\n    cudaFree(d_input); cudaFree(d_output);\n    cudaEventDestroy(start); cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Shared memory is on-chip, fast memory\n2. Shared among threads in a block\n3. Declared with __shared__ keyword\n4. Requires __syncthreads() for synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **10_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}