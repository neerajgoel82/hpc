{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Tiled Matrix Multiplication",
    "## Phase 2: Memory Management - Shared Memory",
    "",
    "**Learning Objectives:**",
    "- Implement tiled matrix multiplication",
    "- Use shared memory for optimization",
    "- Understand blocking techniques",
    "- Reduce global memory accesses",
    "- Measure performance improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Tiled Matrix Multiplication",
    "",
    "**Tiling Strategy:**",
    "- Load tiles from global to shared memory",
    "- Perform computations on tiles",
    "- Reduce global memory bandwidth",
    "",
    "**Algorithm:**",
    "1. Each block loads TILE_SIZE x TILE_SIZE elements",
    "2. Compute partial products using shared memory",
    "3. Accumulate results",
    "4. Repeat for all tiles",
    "",
    "**Performance:**",
    "- Reduces global memory traffic",
    "- Increases arithmetic intensity",
    "- Typical speedup: 5-10x over naive implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Naive Matrix Multiplication (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n#include <math.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n\n#define TILE_SIZE 16\n\n__global__ void tiledMatMulKernel(const float* A, const float* B, float* C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; t++) {\n        if (row < N && t * TILE_SIZE + threadIdx.x < N)\n            As[threadIdx.y][threadIdx.x] = A[row * N + t * TILE_SIZE + threadIdx.x];\n        else\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n\n        if (col < N && t * TILE_SIZE + threadIdx.y < N)\n            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        else\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; k++)\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N)\n        C[row * N + col] = sum;\n}\n\nint main() {\n    printf(\"=== Tiled Matrix Multiplication ===\\n\\n\");\n    const int N = 512;\n    size_t bytes = N * N * sizeof(float);\n\n    float *h_A = (float*)malloc(bytes);\n    float *h_B = (float*)malloc(bytes);\n    float *h_C = (float*)malloc(bytes);\n\n    for (int i = 0; i < N * N; i++) {\n        h_A[i] = rand() / (float)RAND_MAX;\n        h_B[i] = rand() / (float)RAND_MAX;\n    }\n\n    float *d_A, *d_B, *d_C;\n    CUDA_CHECK(cudaMalloc(&d_A, bytes));\n    CUDA_CHECK(cudaMalloc(&d_B, bytes));\n    CUDA_CHECK(cudaMalloc(&d_C, bytes));\n    CUDA_CHECK(cudaMemcpy(d_A, h_A, bytes, cudaMemcpyHostToDevice));\n    CUDA_CHECK(cudaMemcpy(d_B, h_B, bytes, cudaMemcpyHostToDevice));\n\n    dim3 threads(TILE_SIZE, TILE_SIZE);\n    dim3 blocks((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n    CUDA_CHECK(cudaEventRecord(start));\n\n    tiledMatMulKernel<<<blocks, threads>>>(d_A, d_B, d_C, N);\n\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaMemcpy(h_C, d_C, bytes, cudaMemcpyDeviceToHost));\n    CUDA_CHECK(cudaEventSynchronize(stop));\n\n    float ms;\n    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n    printf(\"Matrix size: %dx%d, Tile: %dx%d\\n\", N, N, TILE_SIZE, TILE_SIZE);\n    printf(\"Time: %.3f ms, GFLOPS: %.2f\\n\", ms, (2.0*N*N*N)/(ms*1e6));\n\n    free(h_A); free(h_B); free(h_C);\n    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n    cudaEventDestroy(start); cudaEventDestroy(stop);\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Tiled Matrix Multiplication with Shared Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n#define TILE_SIZE 16\n\n__global__ void matMulTiled(float *A, float *B, float *C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n\n    int row = by * TILE_SIZE + ty;\n    int col = bx * TILE_SIZE + tx;\n\n    float sum = 0.0f;\n\n    // Loop over tiles\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; t++) {\n        // Load tile into shared memory\n        if (row < N && (t * TILE_SIZE + tx) < N)\n            As[ty][tx] = A[row * N + t * TILE_SIZE + tx];\n        else\n            As[ty][tx] = 0.0f;\n\n        if (col < N && (t * TILE_SIZE + ty) < N)\n            Bs[ty][tx] = B[(t * TILE_SIZE + ty) * N + col];\n        else\n            Bs[ty][tx] = 0.0f;\n\n        __syncthreads();\n\n        // Compute partial product\n        for (int k = 0; k < TILE_SIZE; k++) {\n            sum += As[ty][k] * Bs[k][tx];\n        }\n\n        __syncthreads();\n    }\n\n    // Write result\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\nint main() {\n    int N = 1024;\n    size_t size = N * N * sizeof(float);\n\n    printf(\"Matrix Multiplication: %dx%d\\",
    "\", N, N);\n\n    float *h_A = (float*)malloc(size);\n    float *h_B = (float*)malloc(size);\n    float *h_C = (float*)malloc(size);\n\n    for (int i = 0; i < N * N; i++) {\n        h_A[i] = rand() / (float)RAND_MAX;\n        h_B[i] = rand() / (float)RAND_MAX;\n    }\n\n    float *d_A, *d_B, *d_C;\n    cudaMalloc(&d_A, size);\n    cudaMalloc(&d_B, size);\n    cudaMalloc(&d_C, size);\n\n    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n\n    dim3 blockDim(TILE_SIZE, TILE_SIZE);\n    dim3 gridDim((N + TILE_SIZE - 1) / TILE_SIZE,\n                 (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    cudaEventRecord(start);\n    matMulTiled<<<gridDim, blockDim>>>(d_A, d_B, d_C, N);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float milliseconds = 0;\n    cudaEventElapsedTime(&milliseconds, start, stop);\n\n    printf(\"\\",
    "Tiled Matrix Multiplication (Tile Size: %d)\\",
    "\", TILE_SIZE);\n    printf(\"Time: %.3f ms\\",
    "\", milliseconds);\n    printf(\"GFLOPS: %.2f\\",
    "\", (2.0 * N * N * N) / (milliseconds * 1e6));\n\n    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);\n\n    free(h_A); free(h_B); free(h_C);\n    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Comparing Naive vs Tiled Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n#define TILE_SIZE 16\n\n__global__ void matMulNaive(float *A, float *B, float *C, int N) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        float sum = 0.0f;\n        for (int k = 0; k < N; k++) {\n            sum += A[row * N + k] * B[k * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n__global__ void matMulTiled(float *A, float *B, float *C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int row = blockIdx.y * TILE_SIZE + threadIdx.y;\n    int col = blockIdx.x * TILE_SIZE + threadIdx.x;\n\n    float sum = 0.0f;\n\n    for (int t = 0; t < (N + TILE_SIZE - 1) / TILE_SIZE; t++) {\n        if (row < N && (t * TILE_SIZE + threadIdx.x) < N)\n            As[threadIdx.y][threadIdx.x] = A[row * N + t * TILE_SIZE + threadIdx.x];\n        else\n            As[threadIdx.y][threadIdx.x] = 0.0f;\n\n        if (col < N && (t * TILE_SIZE + threadIdx.y) < N)\n            Bs[threadIdx.y][threadIdx.x] = B[(t * TILE_SIZE + threadIdx.y) * N + col];\n        else\n            Bs[threadIdx.y][threadIdx.x] = 0.0f;\n\n        __syncthreads();\n\n        for (int k = 0; k < TILE_SIZE; k++) {\n            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = sum;\n    }\n}\n\nint main() {\n    int sizes[] = {128, 256, 512, 1024};\n\n    printf(\"Performance Comparison: Naive vs Tiled Matrix Multiplication\\",
    "\");\n    printf(\"================================================================\\",
    "\\",
    "\");\n\n    for (int s = 0; s < 4; s++) {\n        int N = sizes[s];\n        size_t size = N * N * sizeof(float);\n\n        float *h_A = (float*)malloc(size);\n        float *h_B = (float*)malloc(size);\n        float *h_C = (float*)malloc(size);\n\n        for (int i = 0; i < N * N; i++) {\n            h_A[i] = 1.0f;\n            h_B[i] = 1.0f;\n        }\n\n        float *d_A, *d_B, *d_C;\n        cudaMalloc(&d_A, size);\n        cudaMalloc(&d_B, size);\n        cudaMalloc(&d_C, size);\n\n        cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);\n        cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);\n\n        dim3 blockDim(16, 16);\n        dim3 gridDim((N + 15) / 16, (N + 15) / 16);\n\n        cudaEvent_t start, stop;\n        cudaEventCreate(&start);\n        cudaEventCreate(&stop);\n\n        // Naive version\n        cudaEventRecord(start);\n        matMulNaive<<<gridDim, blockDim>>>(d_A, d_B, d_C, N);\n        cudaEventRecord(stop);\n        cudaEventSynchronize(stop);\n\n        float naiveTime = 0;\n        cudaEventElapsedTime(&naiveTime, start, stop);\n\n        // Tiled version\n        cudaEventRecord(start);\n        matMulTiled<<<gridDim, blockDim>>>(d_A, d_B, d_C, N);\n        cudaEventRecord(stop);\n        cudaEventSynchronize(stop);\n\n        float tiledTime = 0;\n        cudaEventElapsedTime(&tiledTime, start, stop);\n\n        float speedup = naiveTime / tiledTime;\n\n        printf(\"Size: %dx%d\\",
    "\", N, N);\n        printf(\"  Naive: %.3f ms (%.2f GFLOPS)\\",
    "\",\n               naiveTime, (2.0 * N * N * N) / (naiveTime * 1e6));\n        printf(\"  Tiled: %.3f ms (%.2f GFLOPS)\\",
    "\",\n               tiledTime, (2.0 * N * N * N) / (tiledTime * 1e6));\n        printf(\"  Speedup: %.2fx\\",
    "\\",
    "\", speedup);\n\n        free(h_A); free(h_B); free(h_C);\n        cudaFree(d_A); cudaFree(d_B); cudaFree(d_c);\n        cudaEventDestroy(start);\n        cudaEventDestroy(stop);\n    }\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise\n",
    "\n",
    "**Exercise 1:** Modify the tile size and measure performance impact\n",
    "\n",
    "**Exercise 2:** Implement verification to compare naive vs tiled results\n",
    "\n",
    "**Exercise 3:** Add support for non-square matrices\n",
    "\n",
    "**Exercise 4:** Profile memory access patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Tiled Matrix Multiplication ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Tiling** reduces global memory accesses dramatically\n",
    "2. **Shared memory** is ~100x faster than global memory\n",
    "3. **__syncthreads()** ensures all threads have loaded data\n",
    "4. Tile size affects occupancy and performance\n",
    "5. Typical speedup: 5-10x over naive implementation\n",
    "6. Memory reuse is key to GPU performance\n",
    "7. Blocking is a fundamental optimization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll learn:\n",
    "- Memory coalescing patterns\n",
    "- Aligned vs unaligned access\n",
    "- Measuring memory bandwidth\n",
    "- Optimizing access patterns\n",
    "\n",
    "Continue to: **11_coalescing_demo.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "*Use this space to write your own notes and observations:*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}