{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 21: Atomic Operations Patterns",
    "## Phase 4: Advanced Memory & Synchronization",
    "",
    "**Learning Objectives:**",
    "- Understand atomic operations",
    "- Learn atomicAdd",
    "- Master atomicCAS",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Atomic Operations Patterns",
    "",
    "**Topics Covered:**",
    "- atomic operations",
    "- atomicAdd",
    "- atomicCAS",
    "- thread-safety",
    "",
    "**Key Concepts:**",
    "This notebook covers atomic operations in the context of Phase 4: Advanced Memory & Synchronization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Atomic Operations Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n#include <math.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n\n__global__ void atomicAddKernel(int* counter, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) atomicAdd(counter, 1);\n}\n\n__global__ void atomicMinMaxKernel(const int* data, int* min_val, int* max_val, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        atomicMin(min_val, data[idx]);\n        atomicMax(max_val, data[idx]);\n    }\n}\n\nint main() {\n    printf(\"=== Atomic Operations ===\\n\\n\");\n    const int N = 1 << 20;\n\n    printf(\"Test 1: atomicAdd\\n\");\n    int *d_counter;\n    CUDA_CHECK(cudaMalloc(&d_counter, sizeof(int)));\n    CUDA_CHECK(cudaMemset(d_counter, 0, sizeof(int)));\n\n    atomicAddKernel<<<(N + 255) / 256, 256>>>(d_counter, N);\n\n    int h_counter;\n    CUDA_CHECK(cudaMemcpy(&h_counter, d_counter, sizeof(int), cudaMemcpyDeviceToHost));\n    printf(\"  Counter: %d (expected %d) - %s\\n\\n\", h_counter, N,\n           (h_counter == N) ? \"PASS\" : \"FAIL\");\n\n    printf(\"Test 2: atomicMin/Max\\n\");\n    int *h_data = (int*)malloc(N * sizeof(int));\n    for (int i = 0; i < N; i++) h_data[i] = rand() % 1000;\n\n    int *d_data, *d_min, *d_max;\n    CUDA_CHECK(cudaMalloc(&d_data, N * sizeof(int)));\n    CUDA_CHECK(cudaMalloc(&d_min, sizeof(int)));\n    CUDA_CHECK(cudaMalloc(&d_max, sizeof(int)));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, N * sizeof(int), cudaMemcpyHostToDevice));\n\n    int init_min = 999999, init_max = -1;\n    CUDA_CHECK(cudaMemcpy(d_min, &init_min, sizeof(int), cudaMemcpyHostToDevice));\n    CUDA_CHECK(cudaMemcpy(d_max, &init_max, sizeof(int), cudaMemcpyHostToDevice));\n\n    atomicMinMaxKernel<<<(N + 255) / 256, 256>>>(d_data, d_min, d_max, N);\n\n    int h_min, h_max;\n    CUDA_CHECK(cudaMemcpy(&h_min, d_min, sizeof(int), cudaMemcpyDeviceToHost));\n    CUDA_CHECK(cudaMemcpy(&h_max, d_max, sizeof(int), cudaMemcpyDeviceToHost));\n\n    int ref_min = h_data[0], ref_max = h_data[0];\n    for (int i = 1; i < N; i++) {\n        if (h_data[i] < ref_min) ref_min = h_data[i];\n        if (h_data[i] > ref_max) ref_max = h_data[i];\n    }\n\n    printf(\"  Min: %d (expected %d) - %s\\n\", h_min, ref_min,\n           (h_min == ref_min) ? \"PASS\" : \"FAIL\");\n    printf(\"  Max: %d (expected %d) - %s\\n\", h_max, ref_max,\n           (h_max == ref_max) ? \"PASS\" : \"FAIL\");\n\n    free(h_data);\n    cudaFree(d_counter); cudaFree(d_data); cudaFree(d_min); cudaFree(d_max);\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Atomics ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Atomic operations ensure thread-safe updates\n2. Useful for histograms, reductions, locks\n3. Can create contention bottlenecks\n4. Types: atomicAdd, atomicMax, atomicCAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **22_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}