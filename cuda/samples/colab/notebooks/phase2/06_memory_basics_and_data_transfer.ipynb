{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: Memory Basics and Data Transfer",
    "## Phase 2: Memory Management",
    "",
    "**Learning Objectives:**",
    "- Master CUDA memory allocation and deallocation",
    "- Understand different memory transfer patterns",
    "- Learn about pinned (page-locked) memory",
    "- Measure memory transfer bandwidth",
    "- Optimize host-device data movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: CUDA Memory Model",
    "",
    "**Memory Types:**",
    "- **Global Memory**: Large, slow, accessible by all threads",
    "- **Pinned Memory**: Non-pageable host memory for faster transfers",
    "- **Device Memory**: GPU VRAM",
    "",
    "**Memory Functions:**",
    "```cuda",
    "cudaMalloc(&d_ptr, size);           // Allocate device memory",
    "cudaFree(d_ptr);                    // Free device memory",
    "cudaMemcpy(dst, src, size, kind);   // Copy memory",
    "cudaMallocHost(&h_ptr, size);       // Allocate pinned memory",
    "cudaFreeHost(h_ptr);                // Free pinned memory",
    "```",
    "",
    "**Transfer Bandwidth:**",
    "- Pinned memory: ~12 GB/s (PCIe 3.0 x16)",
    "- Pageable memory: ~6 GB/s",
    "- Async transfers possible with pinned memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Memory Basics and Data Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n                   cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\nint main() {\n    int n = 1000;\n    size_t size = n * sizeof(float);\n    \n    printf(\"=== Basic Memory Allocation Example ===\\n\");\n    printf(\"Array size: %d elements (%zu bytes)\\n\\n\", n, size);\n    \n    // Step 1: Allocate host (CPU) memory\n    float *h_data = (float*)malloc(size);\n    if (h_data == NULL) {\n        printf(\"Failed to allocate host memory\\n\");\n        return 1;\n    }\n    printf(\"\u2713 Host memory allocated: %p\\n\", h_data);\n    \n    // Initialize host data\n    for (int i = 0; i < n; i++) {\n        h_data[i] = i * 1.0f;\n    }\n    printf(\"\u2713 Host data initialized (first 5: %.1f, %.1f, %.1f, %.1f, %.1f)\\n\",\n           h_data[0], h_data[1], h_data[2], h_data[3], h_data[4]);\n    \n    // Step 2: Allocate device (GPU) memory\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    printf(\"\u2713 Device memory allocated: %p\\n\", d_data);\n    \n    // Step 3: Copy host to device\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n    printf(\"\u2713 Data copied from host to device\\n\");\n    \n    // Step 4: Verify by copying back\n    float *h_verify = (float*)malloc(size);\n    CUDA_CHECK(cudaMemcpy(h_verify, d_data, size, cudaMemcpyDeviceToHost));\n    printf(\"\u2713 Data copied back for verification\\n\");\n    \n    // Verify data integrity\n    bool correct = true;\n    for (int i = 0; i < n; i++) {\n        if (h_verify[i] != h_data[i]) {\n            printf(\"\u2717 Mismatch at index %d: %.1f != %.1f\\n\", i, h_verify[i], h_data[i]);\n            correct = false;\n            break;\n        }\n    }\n    \n    if (correct) {\n        printf(\"\u2713 Data transfer successful! All values match.\\n\");\n    }\n    \n    // Step 5: Clean up\n    CUDA_CHECK(cudaFree(d_data));\n    free(h_data);\n    free(h_verify);\n    printf(\"\\n\u2713 Memory freed successfully\\n\");\n    \n    return 0;\n}"
  },
  {
   "cell_type": "markdown",
   "source": "## Example 2: Pinned Memory for Faster Transfers",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%cu\n// Exercise: Implement a function that allocates memory, transfers data, \n// processes it on GPU, and returns the result\n\n#include <stdio.h>\n#include <stdlib.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            printf(\"CUDA error: %s\\n\", cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void scaleArray(float *data, float scale, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] *= scale;\n    }\n}\n\nint main() {\n    // TODO: \n    // 1. Allocate host and device memory\n    // 2. Initialize host data\n    // 3. Transfer to device\n    // 4. Launch kernel to scale array by 2.0\n    // 5. Transfer back and verify results\n    // 6. Clean up memory\n    \n    int n = 1000;\n    size_t size = n * sizeof(float);\n    \n    // Your solution here\n    printf(\"Exercise: Implement complete memory management workflow\\n\");\n    \n    return 0;\n}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n1. **cudaMalloc** allocates memory on the GPU, **cudaFree** releases it\n2. **cudaMemcpy** transfers data; specify direction with cudaMemcpyKind\n3. **Pinned memory** (cudaMallocHost) provides 1.5-2x faster transfer speeds\n4. **Pageable memory** (malloc) is default but slower for GPU transfers\n5. Always check return values and use CUDA_CHECK macro for error handling\n6. **cudaEvent** API provides accurate timing for GPU operations\n7. **Bandwidth = Data Size / Transfer Time** measures transfer efficiency\n8. Minimize host-device transfers - they are expensive operations"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Next Steps\n\nIn the next notebook, we'll learn about:\n- Memory bandwidth benchmarking techniques\n- Measuring effective bandwidth\n- Understanding PCIe transfer limits\n- Optimizing memory transfer patterns\n\nContinue to: **07_memory_bandwidth_benchmarking.ipynb**"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Memory Basics And Data Transfer is essential for CUDA programming\n2. Understanding memory patterns improves performance significantly\n3. Always benchmark and verify results\n4. Use CUDA events for accurate timing\n5. Error checking is critical for production code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **07_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}