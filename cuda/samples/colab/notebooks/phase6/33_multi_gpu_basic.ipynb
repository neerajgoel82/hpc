{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 33: Multi-GPU Programming Basics",
    "## Phase 6: Streams & Concurrency",
    "",
    "**Learning Objectives:**",
    "- Understand multi-GPU",
    "- Learn device management",
    "- Master load balancing",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Multi-GPU Programming Basics",
    "",
    "**Topics Covered:**",
    "- multi-GPU",
    "- device management",
    "- load balancing",
    "",
    "**Key Concepts:**",
    "This notebook covers multi-GPU in the context of Phase 6: Streams & Concurrency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Multi-GPU Programming Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void processKernel(float *data, int n, int offset) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        int globalIdx = offset + idx;\n        data[idx] = sqrtf(globalIdx * 1.0f) * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Multi-GPU Basic Programming ===\\n\\n\");\n\n    int deviceCount;\n    CUDA_CHECK(cudaGetDeviceCount(&deviceCount));\n\n    printf(\"Found %d CUDA device(s)\\n\\n\", deviceCount);\n\n    if (deviceCount < 2) {\n        printf(\"Note: Only %d GPU available.\\n\", deviceCount);\n        printf(\"      Demonstrating multi-GPU pattern anyway...\\n\\n\");\n    }\n\n    int n = 1 << 24;  // Total elements\n    int devicesToUse = (deviceCount >= 2) ? 2 : 1;\n    int nPerDevice = n / devicesToUse;\n    size_t size = nPerDevice * sizeof(float);\n\n    // Allocate on each device\n    float **d_data = (float**)malloc(devicesToUse * sizeof(float*));\n    float **h_data = (float**)malloc(devicesToUse * sizeof(float*));\n\n    cudaEvent_t *start = (cudaEvent_t*)malloc(devicesToUse * sizeof(cudaEvent_t));\n    cudaEvent_t *stop = (cudaEvent_t*)malloc(devicesToUse * sizeof(cudaEvent_t));\n\n    for (int dev = 0; dev < devicesToUse; dev++) {\n        CUDA_CHECK(cudaSetDevice(dev));\n\n        h_data[dev] = (float*)malloc(size);\n        CUDA_CHECK(cudaMalloc(&d_data[dev], size));\n\n        cudaEventCreate(&start[dev]);\n        cudaEventCreate(&stop[dev]);\n    }\n\n    // Process on each GPU\n    for (int dev = 0; dev < devicesToUse; dev++) {\n        CUDA_CHECK(cudaSetDevice(dev));\n\n        int offset = dev * nPerDevice;\n\n        // Initialize\n        for (int i = 0; i < nPerDevice; i++) {\n            h_data[dev][i] = offset + i;\n        }\n\n        CUDA_CHECK(cudaMemcpy(d_data[dev], h_data[dev], size, cudaMemcpyHostToDevice));\n\n        int threads = 256;\n        int blocks = (nPerDevice + threads - 1) / threads;\n\n        cudaEventRecord(start[dev]);\n        processKernel<<<blocks, threads>>>(d_data[dev], nPerDevice, offset);\n        cudaEventRecord(stop[dev]);\n    }\n\n    // Wait and collect results\n    for (int dev = 0; dev < devicesToUse; dev++) {\n        CUDA_CHECK(cudaSetDevice(dev));\n        cudaEventSynchronize(stop[dev]);\n\n        float ms;\n        cudaEventElapsedTime(&ms, start[dev], stop[dev]);\n\n        CUDA_CHECK(cudaMemcpy(h_data[dev], d_data[dev], size, cudaMemcpyDeviceToHost));\n\n        printf(\"GPU %d: %.2f ms, %.2f GB/s\\n\",\n               dev, ms, (size * 2 / 1e9) / (ms / 1000.0));\n    }\n\n    // Verify\n    bool correct = true;\n    for (int dev = 0; dev < devicesToUse; dev++) {\n        int offset = dev * nPerDevice;\n        for (int i = 0; i < 100; i++) {\n            float expected = sqrtf((offset + i) * 1.0f) * 2.0f;\n            if (abs(h_data[dev][i] - expected) > 1e-3) {\n                correct = false;\n                break;\n            }\n        }\n    }\n\n    printf(\"\\nResult: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n\n    // Cleanup\n    for (int dev = 0; dev < devicesToUse; dev++) {\n        CUDA_CHECK(cudaSetDevice(dev));\n        free(h_data[dev]);\n        cudaFree(d_data[dev]);\n        cudaEventDestroy(start[dev]);\n        cudaEventDestroy(stop[dev]);\n    }\n\n    free(h_data);\n    free(d_data);\n    free(start);\n    free(stop);\n\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Multi Gpu Basic ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Scale across multiple GPUs\n2. cudaSetDevice for GPU selection\n3. Data parallelism patterns\n4. P2P transfers between GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **34_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}