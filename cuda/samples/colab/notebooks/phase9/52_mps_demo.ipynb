{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 52: Multi-Process Service (MPS)",
    "## Phase 9: Advanced Topics",
    "",
    "**Learning Objectives:**",
    "- Understand MPS",
    "- Learn multi-process",
    "- Master GPU sharing",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Multi-Process Service (MPS)",
    "",
    "**Topics Covered:**",
    "- MPS",
    "- multi-process",
    "- GPU sharing",
    "",
    "**Key Concepts:**",
    "This notebook covers MPS in the context of Phase 9: Advanced Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Multi-Process Service (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n\n/*\n * Multi-Process Service (MPS) Demo\n *\n * MPS allows multiple processes to share GPU resources efficiently.\n * This sample demonstrates the concept and provides information about MPS.\n *\n * To actually use MPS, you need to:\n * 1. Start MPS daemon: nvidia-cuda-mps-control -d\n * 2. Run multiple processes\n * 3. Stop MPS: echo quit | nvidia-cuda-mps-control\n *\n * Without MPS: Multiple processes time-slice the GPU (poor utilization)\n * With MPS: Multiple processes share GPU simultaneously (better utilization)\n */\n\n#include <stdio.h>\n#include <cuda_runtime.h>\n#include <unistd.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n                    cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Kernel that does some work (matrix-vector multiply)\n__global__ void matVecMul(float *A, float *x, float *y, int n) {\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < n) {\n        float sum = 0.0f;\n        for (int col = 0; col < n; col++) {\n            sum += A[row * n + col] * x[col];\n        }\n        y[row] = sum;\n    }\n}\n\n// Kernel that keeps GPU busy\n__global__ void busyKernel(float *data, int n, int iterations) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < n) {\n        float value = data[idx];\n\n        // Perform many computations to keep GPU busy\n        for (int i = 0; i < iterations; i++) {\n            value = sinf(value) * cosf(value) + 1.0f;\n            value = sqrtf(fabsf(value));\n        }\n\n        data[idx] = value;\n    }\n}\n\nvoid checkMPSStatus() {\n    printf(\"=== MPS Status Check ===\\n\\n\");\n\n    // Check if MPS is available\n    FILE *fp = popen(\"nvidia-smi -q -d COMPUTE\", \"r\");\n    if (fp != NULL) {\n        char line[256];\n        bool foundMPS = false;\n\n        while (fgets(line, sizeof(line), fp) != NULL) {\n            if (strstr(line, \"MPS\") != NULL) {\n                printf(\"%s\", line);\n                foundMPS = true;\n            }\n        }\n\n        pclose(fp);\n\n        if (!foundMPS) {\n            printf(\"MPS status: Not detected or not running\\n\");\n        }\n    } else {\n        printf(\"Could not check MPS status (nvidia-smi not available)\\n\");\n    }\n\n    printf(\"\\n\");\n}\n\nvoid printMPSInfo() {\n    printf(\"=== About CUDA Multi-Process Service (MPS) ===\\n\\n\");\n\n    printf(\"What is MPS?\\n\");\n    printf(\"  MPS is a client-server runtime that allows multiple processes\\n\");\n    printf(\"  to share a single GPU context. This enables better GPU utilization\\n\");\n    printf(\"  when multiple small workloads run concurrently.\\n\\n\");\n\n    printf(\"Benefits:\\n\");\n    printf(\"  - Reduced context switching overhead\\n\");\n    printf(\"  - Better GPU utilization for small kernels\\n\");\n    printf(\"  - Multiple processes can use GPU simultaneously\\n\");\n    printf(\"  - Lower latency for concurrent workloads\\n\\n\");\n\n    printf(\"When to use MPS:\\n\");\n    printf(\"  - Multiple MPI ranks per node\\n\");\n    printf(\"  - Multiple small applications sharing GPU\\n\");\n    printf(\"  - HPC workloads with many processes\\n\");\n    printf(\"  - Microservices architecture\\n\\n\");\n\n    printf(\"How to enable MPS:\\n\");\n    printf(\"  1. Export CUDA_VISIBLE_DEVICES=0 (or your GPU ID)\\n\");\n    printf(\"  2. Start MPS daemon: nvidia-cuda-mps-control -d\\n\");\n    printf(\"  3. Run your applications\\n\");\n    printf(\"  4. Stop MPS: echo quit | nvidia-cuda-mps-control\\n\\n\");\n\n    printf(\"Limitations:\\n\");\n    printf(\"  - Requires Volta or newer for full features\\n\");\n    printf(\"  - Limited debugger support\\n\");\n    printf(\"  - All processes must be from same user\\n\");\n    printf(\"  - Some features require privileged access\\n\\n\");\n}\n\nint main(int argc, char **argv) {\n    printf(\"=== CUDA MPS Demo ===\\n\\n\");\n\n    // Get process ID for identification\n    int pid = getpid();\n    int processNum = 0;\n\n    if (argc > 1) {\n        processNum = atoi(argv[1]);\n    }\n\n    printf(\"Process ID: %d\\n\", pid);\n    printf(\"Process Number: %d\\n\\n\", processNum);\n\n    // Check device properties\n    int device;\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDevice(&device));\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n\n    printf(\"Device: %s\\n\", prop.name);\n    printf(\"Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n    printf(\"Multi-Process Service: %s\\n\\n\",\n           prop.major >= 7 ? \"Fully Supported (Volta+)\" :\n           prop.major >= 3 ? \"Partially Supported\" : \"Not Supported\");\n\n    // Problem size\n    const int N = 2048;  // Matrix size\n    const size_t matrixBytes = N * N * sizeof(float);\n    const size_t vectorBytes = N * sizeof(float);\n\n    // Allocate device memory\n    float *d_A, *d_x, *d_y;\n    CUDA_CHECK(cudaMalloc(&d_A, matrixBytes));\n    CUDA_CHECK(cudaMalloc(&d_x, vectorBytes));\n    CUDA_CHECK(cudaMalloc(&d_y, vectorBytes));\n\n    // Initialize data\n    CUDA_CHECK(cudaMemset(d_A, 0, matrixBytes));\n    CUDA_CHECK(cudaMemset(d_x, 0, vectorBytes));\n\n    // Create events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // Launch parameters\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n\n    printf(\"Running workload for process %d...\\n\", processNum);\n    printf(\"Matrix size: %d x %d\\n\", N, N);\n    printf(\"Grid: %d blocks, Block: %d threads\\n\\n\", blocksPerGrid, threadsPerBlock);\n\n    // Run multiple iterations\n    const int iterations = 50;\n\n    CUDA_CHECK(cudaEventRecord(start));\n\n    for (int i = 0; i < iterations; i++) {\n        matVecMul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_x, d_y, N);\n\n        // Add some busy work\n        busyKernel<<<blocksPerGrid, threadsPerBlock>>>(d_y, N, 100);\n\n        // Small delay to simulate real application\n        if (i % 10 == 0) {\n            CUDA_CHECK(cudaDeviceSynchronize());\n        }\n    }\n\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float ms = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n\n    printf(\"Completed %d iterations\\n\", iterations);\n    printf(\"Total time: %.3f ms\\n\", ms);\n    printf(\"Average time per iteration: %.3f ms\\n\\n\", ms / iterations);\n\n    // Cleanup\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n    CUDA_CHECK(cudaFree(d_A));\n    CUDA_CHECK(cudaFree(d_x));\n    CUDA_CHECK(cudaFree(d_y));\n\n    // Print MPS information\n    if (processNum == 0) {\n        checkMPSStatus();\n        printMPSInfo();\n\n        printf(\"=== To Test MPS ===\\n\\n\");\n        printf(\"Run without MPS:\\n\");\n        printf(\"  Terminal 1: ./mps_demo 0\\n\");\n        printf(\"  Terminal 2: ./mps_demo 1\\n\");\n        printf(\"  (Processes will time-slice the GPU)\\n\\n\");\n\n        printf(\"Run with MPS:\\n\");\n        printf(\"  1. nvidia-cuda-mps-control -d\\n\");\n        printf(\"  2. Terminal 1: ./mps_demo 0\\n\");\n        printf(\"  3. Terminal 2: ./mps_demo 1\\n\");\n        printf(\"  4. echo quit | nvidia-cuda-mps-control\\n\");\n        printf(\"  (Processes will share GPU simultaneously)\\n\\n\");\n\n        printf(\"Compare execution times - MPS should show better total throughput\\n\");\n        printf(\"when running multiple processes concurrently.\\n\");\n    }\n\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Mps Demo ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. MPS = Multi-Process Service\n2. Share GPU among multiple processes\n3. Better utilization of GPU resources\n4. Important for containerized workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **53_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}