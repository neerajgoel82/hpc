{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 54: Tensor Core Programming",
    "## Phase 9: Advanced Topics",
    "",
    "**Learning Objectives:**",
    "- Understand tensor cores",
    "- Learn matrix cores",
    "- Master deep learning",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Tensor Core Programming",
    "",
    "**Topics Covered:**",
    "- tensor cores",
    "- matrix cores",
    "- deep learning",
    "",
    "**Key Concepts:**",
    "This notebook covers tensor cores in the context of Phase 9: Advanced Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Tensor Core Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n\n/*\n * Tensor Cores Demo - Hardware-accelerated matrix operations\n *\n * Tensor Cores are specialized hardware units for matrix multiply-accumulate\n * operations. They provide massive speedup for deep learning workloads.\n *\n * Available on:\n * - Volta (V100): FP16 input, FP32 accumulation\n * - Turing (RTX 20xx): FP16, INT8, INT4\n * - Ampere (A100, RTX 30xx): FP16, BF16, TF32, INT8, INT4, INT1\n * - Hopper (H100): FP8, FP16, BF16, TF32, INT8\n *\n * This sample demonstrates the WMMA (Warp Matrix Multiply-Accumulate) API.\n */\n\n#include <stdio.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\n// Check if WMMA is available (requires sm_70 or higher)\n#if __CUDA_ARCH__ >= 700 || !defined(__CUDA_ARCH__)\n#include <mma.h>\n#define WMMA_AVAILABLE 1\n#else\n#define WMMA_AVAILABLE 0\n#endif\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n                    cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Simple matrix multiply without Tensor Cores (FP32)\n__global__ void matmulFP32(float *A, float *B, float *C, int M, int N, int K) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        float sum = 0.0f;\n        for (int i = 0; i < K; i++) {\n            sum += A[row * K + i] * B[i * N + col];\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n// Matrix multiply with FP16 (no Tensor Cores)\n__global__ void matmulFP16(__half *A, __half *B, float *C, int M, int N, int K) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < M && col < N) {\n        float sum = 0.0f;\n        for (int i = 0; i < K; i++) {\n            float a = __half2float(A[row * K + i]);\n            float b = __half2float(B[i * N + col]);\n            sum += a * b;\n        }\n        C[row * N + col] = sum;\n    }\n}\n\n#if WMMA_AVAILABLE\nusing namespace nvcuda::wmma;\n\n// Matrix multiply using WMMA (Tensor Cores)\n// WMMA works on 16x16x16 matrix fragments\n__global__ void matmulWMMA(__half *A, __half *B, float *C, int M, int N, int K) {\n    // Warp and lane identification\n    int warpM = (blockIdx.y * blockDim.y + threadIdx.y) / 32;\n    int warpN = (blockIdx.x * blockDim.x + threadIdx.x);\n\n    // Declare the fragments\n    fragment<matrix_a, 16, 16, 16, __half, row_major> a_frag;\n    fragment<matrix_b, 16, 16, 16, __half, col_major> b_frag;\n    fragment<accumulator, 16, 16, 16, float> acc_frag;\n    fragment<accumulator, 16, 16, 16, float> c_frag;\n\n    // Initialize the output to zero\n    fill_fragment(acc_frag, 0.0f);\n\n    // Loop over K dimension in chunks of 16\n    for (int k = 0; k < K; k += 16) {\n        int aRow = warpM * 16;\n        int aCol = k;\n        int bRow = k;\n        int bCol = warpN * 16;\n\n        // Bounds checking\n        if (aRow < M && aCol < K && bRow < K && bCol < N) {\n            // Load the inputs\n            load_matrix_sync(a_frag, A + aRow * K + aCol, K);\n            load_matrix_sync(b_frag, B + bRow * N + bCol, N);\n\n            // Perform the matrix multiply-accumulate\n            mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n        }\n    }\n\n    // Store the output\n    int cRow = warpM * 16;\n    int cCol = warpN * 16;\n\n    if (cRow < M && cCol < N) {\n        store_matrix_sync(C + cRow * N + cCol, acc_frag, N, mem_row_major);\n    }\n}\n#endif\n\nvoid printTensorCoreInfo() {\n    int device;\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDevice(&device));\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n\n    printf(\"=== Tensor Core Information ===\\n\\n\");\n    printf(\"Device: %s\\n\", prop.name);\n    printf(\"Compute Capability: %d.%d\\n\\n\", prop.major, prop.minor);\n\n    if (prop.major >= 7) {\n        printf(\"Tensor Cores: AVAILABLE\\n\\n\");\n\n        printf(\"Supported operations by architecture:\\n\");\n        if (prop.major == 7 && prop.minor == 0) {\n            printf(\"  Volta (sm_70):\\n\");\n            printf(\"    - FP16 input, FP32 accumulation\\n\");\n            printf(\"    - D = A * B + C (16x16x16 tiles)\\n\");\n        } else if (prop.major == 7 && prop.minor == 5) {\n            printf(\"  Turing (sm_75):\\n\");\n            printf(\"    - FP16, INT8, INT4, INT1\\n\");\n            printf(\"    - 16x16x16 and 8x8x32 tiles\\n\");\n        } else if (prop.major == 8 && prop.minor == 0) {\n            printf(\"  Ampere (sm_80):\\n\");\n            printf(\"    - FP64, TF32, BF16, FP16, INT8, INT4, INT1\\n\");\n            printf(\"    - Multiple tile sizes\\n\");\n            printf(\"    - Sparsity support\\n\");\n        } else if (prop.major == 8 && prop.minor == 6) {\n            printf(\"  Ampere (sm_86) - Gaming:\\n\");\n            printf(\"    - TF32, BF16, FP16, INT8, INT4, INT1\\n\");\n        } else if (prop.major == 9 && prop.minor == 0) {\n            printf(\"  Hopper (sm_90):\\n\");\n            printf(\"    - FP8, FP64, TF32, BF16, FP16, INT8\\n\");\n            printf(\"    - Thread block clusters\\n\");\n            printf(\"    - Tensor memory accelerator\\n\");\n        }\n\n        printf(\"\\nPerformance characteristics:\\n\");\n        printf(\"  - Up to 8x faster than CUDA cores for FP16\\n\");\n        printf(\"  - Up to 16x faster for INT8\\n\");\n        printf(\"  - Optimized for matrix sizes that are multiples of 16\\n\");\n        printf(\"  - Best with mixed precision (FP16 input, FP32 accumulation)\\n\");\n    } else {\n        printf(\"Tensor Cores: NOT AVAILABLE\\n\");\n        printf(\"  Requires compute capability 7.0 or higher (Volta+)\\n\");\n    }\n    printf(\"\\n\");\n}\n\nint main() {\n    printf(\"=== Tensor Cores Demo ===\\n\\n\");\n\n    printTensorCoreInfo();\n\n    // Check device capabilities\n    int device;\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDevice(&device));\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n\n#if !WMMA_AVAILABLE\n    printf(\"Warning: This code was not compiled with Tensor Core support\\n\");\n    printf(\"Recompile with: nvcc -arch=sm_70 or higher\\n\\n\");\n#endif\n\n    if (prop.major < 7) {\n        printf(\"This GPU does not support Tensor Cores.\\n\");\n        printf(\"Running comparison with standard CUDA cores only.\\n\\n\");\n    }\n\n    // Matrix dimensions (must be multiples of 16 for WMMA)\n    const int M = 1024;  // Rows of A and C\n    const int K = 1024;  // Cols of A, Rows of B\n    const int N = 1024;  // Cols of B and C\n\n    printf(\"Matrix dimensions: M=%d, K=%d, N=%d\\n\", M, K, N);\n    printf(\"Total operations: %.2f GFLOP\\n\\n\", 2.0 * M * N * K / 1e9);\n\n    // Allocate memory\n    size_t bytesA_fp32 = M * K * sizeof(float);\n    size_t bytesB_fp32 = K * N * sizeof(float);\n    size_t bytesC = M * N * sizeof(float);\n    size_t bytesA_fp16 = M * K * sizeof(__half);\n    size_t bytesB_fp16 = K * N * sizeof(__half);\n\n    float *d_A_fp32, *d_B_fp32, *d_C_fp32;\n    __half *d_A_fp16, *d_B_fp16;\n    float *d_C_wmma;\n\n    CUDA_CHECK(cudaMalloc(&d_A_fp32, bytesA_fp32));\n    CUDA_CHECK(cudaMalloc(&d_B_fp32, bytesB_fp32));\n    CUDA_CHECK(cudaMalloc(&d_C_fp32, bytesC));\n    CUDA_CHECK(cudaMalloc(&d_A_fp16, bytesA_fp16));\n    CUDA_CHECK(cudaMalloc(&d_B_fp16, bytesB_fp16));\n    CUDA_CHECK(cudaMalloc(&d_C_wmma, bytesC));\n\n    // Initialize matrices\n    CUDA_CHECK(cudaMemset(d_A_fp32, 0, bytesA_fp32));\n    CUDA_CHECK(cudaMemset(d_B_fp32, 0, bytesB_fp32));\n    CUDA_CHECK(cudaMemset(d_A_fp16, 0, bytesA_fp16));\n    CUDA_CHECK(cudaMemset(d_B_fp16, 0, bytesB_fp16));\n\n    // Create events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // --- Test 1: FP32 Standard CUDA Cores ---\n    printf(\"Test 1: FP32 (Standard CUDA Cores)\\n\");\n\n    dim3 blockDim(16, 16);\n    dim3 gridDim((N + 15) / 16, (M + 15) / 16);\n\n    CUDA_CHECK(cudaEventRecord(start));\n    matmulFP32<<<gridDim, blockDim>>>(d_A_fp32, d_B_fp32, d_C_fp32, M, N, K);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float fp32Time = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&fp32Time, start, stop));\n    float fp32Gflops = 2.0 * M * N * K / fp32Time / 1e6;\n\n    printf(\"  Time: %.3f ms\\n\", fp32Time);\n    printf(\"  Performance: %.2f GFLOPS\\n\\n\", fp32Gflops);\n\n    // --- Test 2: FP16 without Tensor Cores ---\n    printf(\"Test 2: FP16 (CUDA Cores, no Tensor Cores)\\n\");\n\n    CUDA_CHECK(cudaEventRecord(start));\n    matmulFP16<<<gridDim, blockDim>>>(d_A_fp16, d_B_fp16, d_C_wmma, M, N, K);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float fp16Time = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&fp16Time, start, stop));\n    float fp16Gflops = 2.0 * M * N * K / fp16Time / 1e6;\n\n    printf(\"  Time: %.3f ms\\n\", fp16Time);\n    printf(\"  Performance: %.2f GFLOPS\\n\", fp16Gflops);\n    printf(\"  Speedup vs FP32: %.2fx\\n\\n\", fp32Time / fp16Time);\n\n#if WMMA_AVAILABLE\n    // --- Test 3: Tensor Cores with WMMA ---\n    if (prop.major >= 7) {\n        printf(\"Test 3: FP16 with Tensor Cores (WMMA)\\n\");\n\n        // For WMMA, we need different grid dimensions\n        // Each warp processes a 16x16 output tile\n        dim3 wmmaBlockDim(32, 4);  // 128 threads per block\n        dim3 wmmaGridDim((N + 15) / 16, (M + 15) / 16);\n\n        CUDA_CHECK(cudaEventRecord(start));\n        matmulWMMA<<<wmmaGridDim, wmmaBlockDim>>>(d_A_fp16, d_B_fp16, d_C_wmma, M, N, K);\n        CUDA_CHECK(cudaEventRecord(stop));\n        CUDA_CHECK(cudaDeviceSynchronize());\n\n        float wmmaTime = 0;\n        CUDA_CHECK(cudaEventElapsedTime(&wmmaTime, start, stop));\n        float wmmaGflops = 2.0 * M * N * K / wmmaTime / 1e6;\n\n        printf(\"  Time: %.3f ms\\n\", wmmaTime);\n        printf(\"  Performance: %.2f GFLOPS\\n\", wmmaGflops);\n        printf(\"  Speedup vs FP32: %.2fx\\n\", fp32Time / wmmaTime);\n        printf(\"  Speedup vs FP16: %.2fx\\n\\n\", fp16Time / wmmaTime);\n\n        // Summary\n        printf(\"=== Performance Summary ===\\n\");\n        printf(\"FP32 (CUDA Cores):    %6.2f GFLOPS (%.2fx)\\n\", fp32Gflops, 1.0f);\n        printf(\"FP16 (CUDA Cores):    %6.2f GFLOPS (%.2fx)\\n\", fp16Gflops, fp32Gflops / fp16Gflops);\n        printf(\"FP16 (Tensor Cores):  %6.2f GFLOPS (%.2fx)\\n\\n\", wmmaGflops, fp32Gflops / wmmaGflops);\n    }\n#else\n    printf(\"Test 3: Tensor Cores NOT AVAILABLE\\n\");\n    printf(\"  Compile with -arch=sm_70 or higher to enable\\n\\n\");\n#endif\n\n    printf(\"=== Notes ===\\n\");\n    printf(\"Tensor Cores are best for:\\n\");\n    printf(\"  - Deep learning training and inference\\n\");\n    printf(\"  - Large matrix multiplications\\n\");\n    printf(\"  - Mixed precision workloads\\n\");\n    printf(\"  - Batch processing\\n\\n\");\n\n    printf(\"Optimization tips:\\n\");\n    printf(\"  - Use dimensions that are multiples of 16\\n\");\n    printf(\"  - Keep matrices in FP16 for bandwidth\\n\");\n    printf(\"  - Accumulate in FP32 for accuracy\\n\");\n    printf(\"  - Use cuBLAS or cuDNN when possible (highly optimized)\\n\");\n\n    // Cleanup\n    CUDA_CHECK(cudaFree(d_A_fp32));\n    CUDA_CHECK(cudaFree(d_B_fp32));\n    CUDA_CHECK(cudaFree(d_C_fp32));\n    CUDA_CHECK(cudaFree(d_A_fp16));\n    CUDA_CHECK(cudaFree(d_B_fp16));\n    CUDA_CHECK(cudaFree(d_C_wmma));\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Tensor Cores ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Tensor Cores for matrix multiply\n2. FP16 input, FP32 accumulation\n3. 10x-20x speedup for matrix ops\n4. Volta architecture and newer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **55_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}