{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 53: Mixed Precision Computing",
    "## Phase 9: Advanced Topics",
    "",
    "**Learning Objectives:**",
    "- Understand mixed precision",
    "- Learn FP16",
    "- Master FP32",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Mixed Precision Computing",
    "",
    "**Topics Covered:**",
    "- mixed precision",
    "- FP16",
    "- FP32",
    "- accuracy",
    "",
    "**Key Concepts:**",
    "This notebook covers mixed precision in the context of Phase 9: Advanced Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Mixed Precision Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n\n/*\n * Mixed Precision Computing - FP16 and FP32 operations\n *\n * Mixed precision uses both FP16 (half precision) and FP32 (single precision)\n * to optimize both performance and accuracy. FP16 offers:\n * - 2x memory bandwidth\n * - 2x throughput on Tensor Cores\n * - Lower power consumption\n *\n * But requires careful handling to maintain accuracy.\n */\n\n#include <stdio.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n                    cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Vector addition in FP32 (baseline)\n__global__ void vecAddFP32(float *a, float *b, float *c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        c[idx] = a[idx] + b[idx];\n    }\n}\n\n// Vector addition in FP16\n__global__ void vecAddFP16(__half *a, __half *b, __half *c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        c[idx] = __hadd(a[idx], b[idx]);\n    }\n}\n\n// Mixed precision: FP16 input, FP32 accumulation, FP16 output\n__global__ void vecAddMixed(__half *a, __half *b, __half *c, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        // Convert to FP32 for addition\n        float a_fp32 = __half2float(a[idx]);\n        float b_fp32 = __half2float(b[idx]);\n        float sum = a_fp32 + b_fp32;\n        // Convert back to FP16\n        c[idx] = __float2half(sum);\n    }\n}\n\n// Vector dot product in FP32\n__global__ void dotProductFP32(float *a, float *b, float *result, int n) {\n    __shared__ float shared[256];\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int tid = threadIdx.x;\n\n    // Load and multiply\n    float temp = 0.0f;\n    if (idx < n) {\n        temp = a[idx] * b[idx];\n    }\n    shared[tid] = temp;\n    __syncthreads();\n\n    // Reduction in shared memory\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            shared[tid] += shared[tid + stride];\n        }\n        __syncthreads();\n    }\n\n    // Write result\n    if (tid == 0) {\n        atomicAdd(result, shared[0]);\n    }\n}\n\n// Vector dot product - Mixed precision (FP16 input, FP32 accumulation)\n__global__ void dotProductMixed(__half *a, __half *b, float *result, int n) {\n    __shared__ float shared[256];\n\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int tid = threadIdx.x;\n\n    // Load, convert to FP32, and multiply\n    float temp = 0.0f;\n    if (idx < n) {\n        float a_fp32 = __half2float(a[idx]);\n        float b_fp32 = __half2float(b[idx]);\n        temp = a_fp32 * b_fp32;\n    }\n    shared[tid] = temp;\n    __syncthreads();\n\n    // Reduction in FP32\n    for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n        if (tid < stride) {\n            shared[tid] += shared[tid + stride];\n        }\n        __syncthreads();\n    }\n\n    // Write result\n    if (tid == 0) {\n        atomicAdd(result, shared[0]);\n    }\n}\n\n// Matrix multiplication in FP32\n__global__ void matMulFP32(float *A, float *B, float *C, int n) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < n && col < n) {\n        float sum = 0.0f;\n        for (int k = 0; k < n; k++) {\n            sum += A[row * n + k] * B[k * n + col];\n        }\n        C[row * n + col] = sum;\n    }\n}\n\n// Matrix multiplication - Mixed precision\n__global__ void matMulMixed(__half *A, __half *B, float *C, int n) {\n    int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < n && col < n) {\n        float sum = 0.0f;\n        for (int k = 0; k < n; k++) {\n            float a_val = __half2float(A[row * n + k]);\n            float b_val = __half2float(B[k * n + col]);\n            sum += a_val * b_val;\n        }\n        C[row * n + col] = sum;\n    }\n}\n\nint main() {\n    printf(\"=== Mixed Precision Computing Demo ===\\n\\n\");\n\n    // Check device capabilities\n    int device;\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDevice(&device));\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n\n    printf(\"Device: %s\\n\", prop.name);\n    printf(\"Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n\n    // Check FP16 support\n    bool hasFP16 = (prop.major >= 6);\n    if (!hasFP16) {\n        printf(\"Warning: FP16 operations may be slow on this device (requires Pascal or newer)\\n\");\n    }\n    printf(\"\\n\");\n\n    // Problem sizes\n    const int N = 1024 * 1024;  // 1M elements for vectors\n    const int M = 1024;          // 1K x 1K matrix\n\n    // Create events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // --- Test 1: Vector Addition ---\n    printf(\"=== Test 1: Vector Addition ===\\n\");\n    printf(\"Vector size: %d elements\\n\\n\", N);\n\n    size_t vecBytesFP32 = N * sizeof(float);\n    size_t vecBytesFP16 = N * sizeof(__half);\n\n    // Allocate FP32 arrays\n    float *d_a_fp32, *d_b_fp32, *d_c_fp32;\n    CUDA_CHECK(cudaMalloc(&d_a_fp32, vecBytesFP32));\n    CUDA_CHECK(cudaMalloc(&d_b_fp32, vecBytesFP32));\n    CUDA_CHECK(cudaMalloc(&d_c_fp32, vecBytesFP32));\n\n    // Allocate FP16 arrays\n    __half *d_a_fp16, *d_b_fp16, *d_c_fp16;\n    CUDA_CHECK(cudaMalloc(&d_a_fp16, vecBytesFP16));\n    CUDA_CHECK(cudaMalloc(&d_b_fp16, vecBytesFP16));\n    CUDA_CHECK(cudaMalloc(&d_c_fp16, vecBytesFP16));\n\n    // Initialize with some values\n    CUDA_CHECK(cudaMemset(d_a_fp32, 0, vecBytesFP32));\n    CUDA_CHECK(cudaMemset(d_b_fp32, 0, vecBytesFP32));\n    CUDA_CHECK(cudaMemset(d_a_fp16, 0, vecBytesFP16));\n    CUDA_CHECK(cudaMemset(d_b_fp16, 0, vecBytesFP16));\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n\n    // FP32 addition\n    printf(\"FP32 Vector Addition:\\n\");\n    CUDA_CHECK(cudaEventRecord(start));\n    vecAddFP32<<<blocksPerGrid, threadsPerBlock>>>(d_a_fp32, d_b_fp32, d_c_fp32, N);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float fp32Time = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&fp32Time, start, stop));\n    printf(\"  Time: %.3f ms\\n\", fp32Time);\n    printf(\"  Bandwidth: %.2f GB/s\\n\", 3.0 * vecBytesFP32 / fp32Time / 1e6);\n\n    // FP16 addition\n    printf(\"\\nFP16 Vector Addition:\\n\");\n    CUDA_CHECK(cudaEventRecord(start));\n    vecAddFP16<<<blocksPerGrid, threadsPerBlock>>>(d_a_fp16, d_b_fp16, d_c_fp16, N);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float fp16Time = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&fp16Time, start, stop));\n    printf(\"  Time: %.3f ms\\n\", fp16Time);\n    printf(\"  Bandwidth: %.2f GB/s\\n\", 3.0 * vecBytesFP16 / fp16Time / 1e6);\n    printf(\"  Speedup: %.2fx\\n\", fp32Time / fp16Time);\n    printf(\"  Memory saved: %.2f MB (%.1f%%)\\n\",\n           (vecBytesFP32 - vecBytesFP16) / 1e6 * 3, 50.0);\n\n    // Mixed precision addition\n    printf(\"\\nMixed Precision Vector Addition:\\n\");\n    CUDA_CHECK(cudaEventRecord(start));\n    vecAddMixed<<<blocksPerGrid, threadsPerBlock>>>(d_a_fp16, d_b_fp16, d_c_fp16, N);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float mixedTime = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&mixedTime, start, stop));\n    printf(\"  Time: %.3f ms\\n\", mixedTime);\n    printf(\"  Speedup: %.2fx\\n\\n\", fp32Time / mixedTime);\n\n    // --- Test 2: Dot Product ---\n    printf(\"=== Test 2: Dot Product ===\\n\");\n\n    float *d_result_fp32;\n    CUDA_CHECK(cudaMalloc(&d_result_fp32, sizeof(float)));\n\n    // FP32 dot product\n    printf(\"FP32 Dot Product:\\n\");\n    CUDA_CHECK(cudaMemset(d_result_fp32, 0, sizeof(float)));\n    CUDA_CHECK(cudaEventRecord(start));\n    dotProductFP32<<<blocksPerGrid, threadsPerBlock>>>(d_a_fp32, d_b_fp32, d_result_fp32, N);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    CUDA_CHECK(cudaEventElapsedTime(&fp32Time, start, stop));\n    printf(\"  Time: %.3f ms\\n\", fp32Time);\n\n    // Mixed precision dot product\n    printf(\"\\nMixed Precision Dot Product:\\n\");\n    CUDA_CHECK(cudaMemset(d_result_fp32, 0, sizeof(float)));\n    CUDA_CHECK(cudaEventRecord(start));\n    dotProductMixed<<<blocksPerGrid, threadsPerBlock>>>(d_a_fp16, d_b_fp16, d_result_fp32, N);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    CUDA_CHECK(cudaEventElapsedTime(&mixedTime, start, stop));\n    printf(\"  Time: %.3f ms\\n\", mixedTime);\n    printf(\"  Speedup: %.2fx\\n\\n\", fp32Time / mixedTime);\n\n    // --- Test 3: Matrix Multiplication ---\n    printf(\"=== Test 3: Matrix Multiplication ===\\n\");\n    printf(\"Matrix size: %d x %d\\n\\n\", M, M);\n\n    size_t matBytesFP32 = M * M * sizeof(float);\n    size_t matBytesFP16 = M * M * sizeof(__half);\n\n    // Allocate matrices\n    float *d_A_fp32, *d_B_fp32, *d_C_fp32;\n    CUDA_CHECK(cudaMalloc(&d_A_fp32, matBytesFP32));\n    CUDA_CHECK(cudaMalloc(&d_B_fp32, matBytesFP32));\n    CUDA_CHECK(cudaMalloc(&d_C_fp32, matBytesFP32));\n\n    __half *d_A_fp16, *d_B_fp16;\n    CUDA_CHECK(cudaMalloc(&d_A_fp16, matBytesFP16));\n    CUDA_CHECK(cudaMalloc(&d_B_fp16, matBytesFP16));\n\n    // Initialize\n    CUDA_CHECK(cudaMemset(d_A_fp32, 0, matBytesFP32));\n    CUDA_CHECK(cudaMemset(d_B_fp32, 0, matBytesFP32));\n    CUDA_CHECK(cudaMemset(d_A_fp16, 0, matBytesFP16));\n    CUDA_CHECK(cudaMemset(d_B_fp16, 0, matBytesFP16));\n\n    dim3 blockDim(16, 16);\n    dim3 gridDim((M + 15) / 16, (M + 15) / 16);\n\n    // FP32 matrix multiply\n    printf(\"FP32 Matrix Multiply:\\n\");\n    CUDA_CHECK(cudaEventRecord(start));\n    matMulFP32<<<gridDim, blockDim>>>(d_A_fp32, d_B_fp32, d_C_fp32, M);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    CUDA_CHECK(cudaEventElapsedTime(&fp32Time, start, stop));\n    float fp32Gflops = 2.0 * M * M * M / fp32Time / 1e6;\n    printf(\"  Time: %.3f ms\\n\", fp32Time);\n    printf(\"  Performance: %.2f GFLOPS\\n\", fp32Gflops);\n\n    // Mixed precision matrix multiply\n    printf(\"\\nMixed Precision Matrix Multiply:\\n\");\n    CUDA_CHECK(cudaEventRecord(start));\n    matMulMixed<<<gridDim, blockDim>>>(d_A_fp16, d_B_fp16, d_C_fp32, M);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    CUDA_CHECK(cudaEventElapsedTime(&mixedTime, start, stop));\n    float mixedGflops = 2.0 * M * M * M / mixedTime / 1e6;\n    printf(\"  Time: %.3f ms\\n\", mixedTime);\n    printf(\"  Performance: %.2f GFLOPS\\n\", mixedGflops);\n    printf(\"  Speedup: %.2fx\\n\", fp32Time / mixedTime);\n    printf(\"  Input memory saved: %.2f MB (%.1f%%)\\n\\n\",\n           (matBytesFP32 - matBytesFP16) / 1e6 * 2, 50.0);\n\n    // Summary\n    printf(\"=== Summary ===\\n\");\n    printf(\"Mixed precision benefits:\\n\");\n    printf(\"  - 2x memory bandwidth (stores half the data)\\n\");\n    printf(\"  - Faster computation (especially on Tensor Cores)\\n\");\n    printf(\"  - Lower power consumption\\n\\n\");\n\n    printf(\"Best practices:\\n\");\n    printf(\"  - Use FP16 for storage and bandwidth-bound operations\\n\");\n    printf(\"  - Use FP32 for accumulation to maintain accuracy\\n\");\n    printf(\"  - Use loss scaling in deep learning to prevent underflow\\n\");\n    printf(\"  - Profile to find the right balance for your application\\n\");\n\n    // Cleanup\n    CUDA_CHECK(cudaFree(d_a_fp32));\n    CUDA_CHECK(cudaFree(d_b_fp32));\n    CUDA_CHECK(cudaFree(d_c_fp32));\n    CUDA_CHECK(cudaFree(d_a_fp16));\n    CUDA_CHECK(cudaFree(d_b_fp16));\n    CUDA_CHECK(cudaFree(d_c_fp16));\n    CUDA_CHECK(cudaFree(d_result_fp32));\n    CUDA_CHECK(cudaFree(d_A_fp32));\n    CUDA_CHECK(cudaFree(d_B_fp32));\n    CUDA_CHECK(cudaFree(d_C_fp32));\n    CUDA_CHECK(cudaFree(d_A_fp16));\n    CUDA_CHECK(cudaFree(d_B_fp16));\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Mixed Precision ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Mix FP32, FP16, and INT8 datatypes\n2. Higher throughput with lower precision\n3. Tensor Cores require FP16/INT8\n4. Accuracy vs speed trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **54_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}