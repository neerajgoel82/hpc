{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 50: Dynamic Parallelism",
    "## Phase 9: Advanced Topics",
    "",
    "**Learning Objectives:**",
    "- Understand dynamic parallelism",
    "- Learn nested kernels",
    "- Master recursion",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Dynamic Parallelism",
    "",
    "**Topics Covered:**",
    "- dynamic parallelism",
    "- nested kernels",
    "- recursion",
    "",
    "**Key Concepts:**",
    "This notebook covers dynamic parallelism in the context of Phase 9: Advanced Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Dynamic Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n\n/*\n * Dynamic Parallelism - Parent kernel launches child kernels\n *\n * This demonstrates CUDA Dynamic Parallelism where GPU kernels can launch\n * other kernels directly without CPU involvement. This is useful for\n * recursive algorithms, adaptive mesh refinement, and dynamic workloads.\n *\n * Compile with: nvcc -arch=sm_35 -rdc=true 50_dynamic_parallelism.cu -o dynamic_parallelism\n * Note: Requires compute capability 3.5 or higher\n */\n\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n                    cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Child kernel - processes a sub-range of data\n__global__ void childKernel(int *data, int offset, int size) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < size) {\n        // Simple computation: square the index and add offset\n        data[offset + idx] = (idx * idx) + offset;\n    }\n}\n\n// Parent kernel - launches multiple child kernels\n__global__ void parentKernel(int *data, int n, int childSize) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int numChildren = (n + childSize - 1) / childSize;\n\n    if (idx < numChildren) {\n        int offset = idx * childSize;\n        int size = min(childSize, n - offset);\n\n        // Calculate grid and block dimensions for child kernel\n        int threadsPerBlock = 256;\n        int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;\n\n        // Launch child kernel from GPU\n        childKernel<<<blocksPerGrid, threadsPerBlock>>>(data, offset, size);\n\n        // Child kernel launches are asynchronous, but we can sync if needed\n        // cudaDeviceSynchronize() would wait for child to complete\n    }\n}\n\n// Verification kernel - simple operation for comparison\n__global__ void simpleKernel(int *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < n) {\n        // Same computation as child kernel\n        data[idx] = (idx * idx);\n    }\n}\n\nint main() {\n    printf(\"=== CUDA Dynamic Parallelism Demo ===\\n\\n\");\n\n    // Check if device supports dynamic parallelism\n    int device;\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDevice(&device));\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n\n    printf(\"Device: %s\\n\", prop.name);\n    printf(\"Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n\n    if (prop.major < 3 || (prop.major == 3 && prop.minor < 5)) {\n        printf(\"Error: Dynamic Parallelism requires compute capability 3.5 or higher\\n\");\n        return 1;\n    }\n\n    // Problem size\n    const int N = 1024 * 1024;  // 1M elements\n    const int childSize = 256;   // Each child processes 256 elements\n    const size_t bytes = N * sizeof(int);\n\n    printf(\"Array size: %d elements\\n\", N);\n    printf(\"Child kernel size: %d elements\\n\\n\", childSize);\n\n    // Allocate host memory\n    int *h_data = (int*)malloc(bytes);\n    int *h_verify = (int*)malloc(bytes);\n\n    // Allocate device memory\n    int *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, bytes));\n\n    // Initialize data\n    for (int i = 0; i < N; i++) {\n        h_data[i] = 0;\n    }\n\n    // Create events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // --- Test 1: Dynamic Parallelism ---\n    printf(\"Test 1: Dynamic Parallelism\\n\");\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, bytes, cudaMemcpyHostToDevice));\n\n    int numParents = (N + childSize - 1) / childSize;\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (numParents + threadsPerBlock - 1) / threadsPerBlock;\n\n    printf(\"Launching %d parent threads in %d blocks\\n\", numParents, blocksPerGrid);\n\n    CUDA_CHECK(cudaEventRecord(start));\n    parentKernel<<<blocksPerGrid, threadsPerBlock>>>(d_data, N, childSize);\n    CUDA_CHECK(cudaEventRecord(stop));\n\n    CUDA_CHECK(cudaDeviceSynchronize());\n    CUDA_CHECK(cudaEventSynchronize(stop));\n\n    float ms = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n    printf(\"Time: %.3f ms\\n\", ms);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, bytes, cudaMemcpyDeviceToHost));\n\n    // --- Test 2: Simple kernel for comparison ---\n    printf(\"\\nTest 2: Simple Kernel (no dynamic parallelism)\\n\");\n    CUDA_CHECK(cudaMemset(d_data, 0, bytes));\n\n    threadsPerBlock = 256;\n    blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n\n    printf(\"Launching %d threads in %d blocks\\n\", N, blocksPerGrid);\n\n    CUDA_CHECK(cudaEventRecord(start));\n    simpleKernel<<<blocksPerGrid, threadsPerBlock>>>(d_data, N);\n    CUDA_CHECK(cudaEventRecord(stop));\n\n    CUDA_CHECK(cudaDeviceSynchronize());\n    CUDA_CHECK(cudaEventSynchronize(stop));\n\n    CUDA_CHECK(cudaEventElapsedTime(&ms, start, stop));\n    printf(\"Time: %.3f ms\\n\", ms);\n\n    CUDA_CHECK(cudaMemcpy(h_verify, d_data, bytes, cudaMemcpyDeviceToHost));\n\n    // Verify results\n    printf(\"\\nVerifying results...\\n\");\n    int errors = 0;\n    for (int i = 0; i < N && errors < 10; i++) {\n        int expected = i * i;\n        if (h_data[i] != expected) {\n            printf(\"Mismatch at %d: got %d, expected %d\\n\", i, h_data[i], expected);\n            errors++;\n        }\n    }\n\n    if (errors == 0) {\n        printf(\"SUCCESS: All results match!\\n\");\n    } else {\n        printf(\"ERRORS: Found %d mismatches\\n\", errors);\n    }\n\n    // Cleanup\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n    CUDA_CHECK(cudaFree(d_data));\n    free(h_data);\n    free(h_verify);\n\n    printf(\"\\nNote: Dynamic parallelism adds overhead. It's beneficial when:\\n\");\n    printf(\"  - Workload is highly irregular or adaptive\\n\");\n    printf(\"  - Cost of CPU-GPU synchronization is high\\n\");\n    printf(\"  - Algorithm is naturally recursive\\n\");\n\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Dynamic Parallelism ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Kernels can launch other kernels\n2. Recursive algorithms on GPU\n3. Adaptive workload generation\n4. Requires compute capability 3.5+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **51_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}