{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 55: WMMA Matrix Multiply",
    "## Phase 9: Advanced Topics",
    "",
    "**Learning Objectives:**",
    "- Understand WMMA",
    "- Learn warp matrix",
    "- Master tensor cores",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: WMMA Matrix Multiply",
    "",
    "**Topics Covered:**",
    "- WMMA",
    "- warp matrix",
    "- tensor cores",
    "",
    "**Key Concepts:**",
    "This notebook covers WMMA in the context of Phase 9: Advanced Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic WMMA Matrix Multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n\n/*\n * WMMA GEMM - Warp Matrix Multiply-Accumulate for Tensor Cores\n *\n * This implements a complete GEMM (General Matrix Multiply) using the\n * WMMA API to leverage Tensor Cores. This is the building block for\n * deep learning operations.\n *\n * Operation: C = alpha * A * B + beta * C\n *\n * WMMA operates on small matrix fragments (typically 16x16x16):\n * - matrix_a: 16x16 input matrix A\n * - matrix_b: 16x16 input matrix B\n * - accumulator: 16x16 accumulator/output matrix C\n *\n * Compile: nvcc -arch=sm_70 55_wmma_gemm.cu -o wmma_gemm\n */\n\n#include <stdio.h>\n#include <cuda_runtime.h>\n#include <cuda_fp16.h>\n\n#if __CUDA_ARCH__ >= 700 || !defined(__CUDA_ARCH__)\n#include <mma.h>\n#define WMMA_AVAILABLE 1\n#else\n#define WMMA_AVAILABLE 0\n#endif\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n                    cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// WMMA tile sizes\nconst int WMMA_M = 16;\nconst int WMMA_N = 16;\nconst int WMMA_K = 16;\n\n#if WMMA_AVAILABLE\nusing namespace nvcuda::wmma;\n\n/*\n * WMMA-based GEMM kernel\n *\n * Each warp computes a WMMA_M x WMMA_N tile of the output matrix.\n * The warp loads tiles from A and B, performs matrix multiply-accumulate,\n * and stores the result to C.\n *\n * Grid and block dimensions:\n * - blockDim: (WARP_SIZE, 4) = 128 threads, 4 warps per block\n * - gridDim: enough blocks to cover the output matrix\n */\n__global__ void wmmaGemm(\n    const __half *A,    // M x K matrix\n    const __half *B,    // K x N matrix\n    float *C,           // M x N matrix\n    int M, int N, int K,\n    float alpha, float beta)\n{\n    // Warp and lane identification\n    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;\n    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);\n\n    // Declare the fragments\n    fragment<matrix_a, WMMA_M, WMMA_N, WMMA_K, __half, row_major> a_frag;\n    fragment<matrix_b, WMMA_M, WMMA_N, WMMA_K, __half, col_major> b_frag;\n    fragment<accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n    fragment<accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n\n    // Initialize accumulator to zero\n    fill_fragment(acc_frag, 0.0f);\n\n    // Calculate base positions for this warp's output tile\n    int cRow = warpM * WMMA_M;\n    int cCol = warpN * WMMA_N;\n\n    // Bounds checking\n    if (cRow >= M || cCol >= N) return;\n\n    // Loop over K dimension in WMMA_K chunks\n    for (int k = 0; k < K; k += WMMA_K) {\n        int aRow = cRow;\n        int aCol = k;\n        int bRow = k;\n        int bCol = cCol;\n\n        // Check bounds for A and B\n        if (aCol + WMMA_K <= K && bRow + WMMA_K <= K) {\n            // Load matrix fragments from A and B\n            load_matrix_sync(a_frag, A + aRow * K + aCol, K);\n            load_matrix_sync(b_frag, B + bRow * N + bCol, N);\n\n            // Perform matrix multiply-accumulate: acc = a * b + acc\n            mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n        }\n    }\n\n    // Load existing C values if beta != 0\n    if (beta != 0.0f) {\n        load_matrix_sync(c_frag, C + cRow * N + cCol, N, mem_row_major);\n\n        // Scale C by beta and add to accumulator\n        for (int i = 0; i < c_frag.num_elements; i++) {\n            acc_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];\n        }\n    } else {\n        // Just scale by alpha\n        for (int i = 0; i < acc_frag.num_elements; i++) {\n            acc_frag.x[i] = alpha * acc_frag.x[i];\n        }\n    }\n\n    // Store the result\n    store_matrix_sync(C + cRow * N + cCol, acc_frag, N, mem_row_major);\n}\n\n/*\n * Optimized WMMA GEMM with tiling for better performance\n *\n * Each warp processes multiple WMMA tiles to improve data reuse\n * and reduce memory traffic.\n */\n__global__ void wmmaGemmTiled(\n    const __half *A,\n    const __half *B,\n    float *C,\n    int M, int N, int K,\n    float alpha, float beta)\n{\n    // Each block processes a larger tile\n    const int BLOCK_ROW_TILES = 4;\n    const int BLOCK_COL_TILES = 4;\n\n    // Warp identification within block\n    int warpId = threadIdx.x / warpSize;\n    int laneId = threadIdx.x % warpSize;\n    int warpRow = warpId / BLOCK_COL_TILES;\n    int warpCol = warpId % BLOCK_COL_TILES;\n\n    // Block position in output matrix\n    int blockRowOffset = blockIdx.x * WMMA_M * BLOCK_ROW_TILES;\n    int blockColOffset = blockIdx.y * WMMA_N * BLOCK_COL_TILES;\n\n    // This warp's position\n    int warpRowOffset = blockRowOffset + warpRow * WMMA_M;\n    int warpColOffset = blockColOffset + warpCol * WMMA_N;\n\n    // Bounds checking\n    if (warpRowOffset >= M || warpColOffset >= N) return;\n\n    // Declare fragments\n    fragment<matrix_a, WMMA_M, WMMA_N, WMMA_K, __half, row_major> a_frag;\n    fragment<matrix_b, WMMA_M, WMMA_N, WMMA_K, __half, col_major> b_frag;\n    fragment<accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n\n    // Initialize accumulator\n    fill_fragment(acc_frag, 0.0f);\n\n    // Main loop over K dimension\n    for (int k = 0; k < K; k += WMMA_K) {\n        if (k + WMMA_K <= K) {\n            // Load tiles\n            load_matrix_sync(a_frag, A + warpRowOffset * K + k, K);\n            load_matrix_sync(b_frag, B + k * N + warpColOffset, N);\n\n            // Multiply-accumulate\n            mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n        }\n    }\n\n    // Apply alpha and beta\n    if (beta != 0.0f) {\n        fragment<accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n        load_matrix_sync(c_frag, C + warpRowOffset * N + warpColOffset, N, mem_row_major);\n\n        for (int i = 0; i < acc_frag.num_elements; i++) {\n            acc_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];\n        }\n    } else {\n        for (int i = 0; i < acc_frag.num_elements; i++) {\n            acc_frag.x[i] = alpha * acc_frag.x[i];\n        }\n    }\n\n    // Store result\n    store_matrix_sync(C + warpRowOffset * N + warpColOffset, acc_frag, N, mem_row_major);\n}\n#endif\n\n// Reference CPU implementation for verification\nvoid cpuGemm(const float *A, const float *B, float *C,\n             int M, int N, int K, float alpha, float beta) {\n    for (int i = 0; i < M; i++) {\n        for (int j = 0; j < N; j++) {\n            float sum = 0.0f;\n            for (int k = 0; k < K; k++) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = alpha * sum + beta * C[i * N + j];\n        }\n    }\n}\n\nint main() {\n    printf(\"=== WMMA GEMM Demo ===\\n\\n\");\n\n    // Check device support\n    int device;\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDevice(&device));\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n\n    printf(\"Device: %s\\n\", prop.name);\n    printf(\"Compute Capability: %d.%d\\n\\n\", prop.major, prop.minor);\n\n    if (prop.major < 7) {\n        printf(\"Error: Tensor Cores require compute capability 7.0 or higher\\n\");\n        return 1;\n    }\n\n#if !WMMA_AVAILABLE\n    printf(\"Error: Code not compiled with WMMA support\\n\");\n    printf(\"Recompile with: nvcc -arch=sm_70 or higher\\n\");\n    return 1;\n#else\n\n    // Matrix dimensions (must be multiples of 16)\n    const int M = 512;\n    const int N = 512;\n    const int K = 512;\n    const float alpha = 1.0f;\n    const float beta = 0.0f;\n\n    printf(\"Matrix dimensions: M=%d, N=%d, K=%d\\n\", M, N, K);\n    printf(\"Operation: C = %.1f * A * B + %.1f * C\\n\", alpha, beta);\n    printf(\"WMMA tile size: %dx%dx%d\\n\", WMMA_M, WMMA_N, WMMA_K);\n    printf(\"Total FLOPs: %.2f GFLOP\\n\\n\", 2.0 * M * N * K / 1e9);\n\n    // Allocate host memory\n    size_t bytesA_fp32 = M * K * sizeof(float);\n    size_t bytesB_fp32 = K * N * sizeof(float);\n    size_t bytesC = M * N * sizeof(float);\n    size_t bytesA_fp16 = M * K * sizeof(__half);\n    size_t bytesB_fp16 = K * N * sizeof(__half);\n\n    float *h_A = (float*)malloc(bytesA_fp32);\n    float *h_B = (float*)malloc(bytesB_fp32);\n    float *h_C = (float*)malloc(bytesC);\n    float *h_C_ref = (float*)malloc(bytesC);\n\n    // Initialize with random values\n    for (int i = 0; i < M * K; i++) {\n        h_A[i] = (float)(rand() % 100) / 100.0f;\n    }\n    for (int i = 0; i < K * N; i++) {\n        h_B[i] = (float)(rand() % 100) / 100.0f;\n    }\n    for (int i = 0; i < M * N; i++) {\n        h_C[i] = 0.0f;\n        h_C_ref[i] = 0.0f;\n    }\n\n    // Convert to FP16\n    __half *h_A_fp16 = (__half*)malloc(bytesA_fp16);\n    __half *h_B_fp16 = (__half*)malloc(bytesB_fp16);\n\n    for (int i = 0; i < M * K; i++) {\n        h_A_fp16[i] = __float2half(h_A[i]);\n    }\n    for (int i = 0; i < K * N; i++) {\n        h_B_fp16[i] = __float2half(h_B[i]);\n    }\n\n    // Allocate device memory\n    __half *d_A, *d_B;\n    float *d_C;\n\n    CUDA_CHECK(cudaMalloc(&d_A, bytesA_fp16));\n    CUDA_CHECK(cudaMalloc(&d_B, bytesB_fp16));\n    CUDA_CHECK(cudaMalloc(&d_C, bytesC));\n\n    // Copy to device\n    CUDA_CHECK(cudaMemcpy(d_A, h_A_fp16, bytesA_fp16, cudaMemcpyHostToDevice));\n    CUDA_CHECK(cudaMemcpy(d_B, h_B_fp16, bytesB_fp16, cudaMemcpyHostToDevice));\n    CUDA_CHECK(cudaMemcpy(d_C, h_C, bytesC, cudaMemcpyHostToDevice));\n\n    // Create events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // --- Test 1: Basic WMMA GEMM ---\n    printf(\"Test 1: Basic WMMA GEMM\\n\");\n\n    dim3 blockDim(32, 4);  // 128 threads per block (4 warps)\n    dim3 gridDim((M + WMMA_M - 1) / WMMA_M, (N + WMMA_N - 1) / WMMA_N);\n\n    printf(\"Grid: (%d, %d), Block: (%d, %d)\\n\", gridDim.x, gridDim.y, blockDim.x, blockDim.y);\n\n    // Warm-up\n    wmmaGemm<<<gridDim, blockDim>>>(d_A, d_B, d_C, M, N, K, alpha, beta);\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    // Timed run\n    CUDA_CHECK(cudaEventRecord(start));\n    wmmaGemm<<<gridDim, blockDim>>>(d_A, d_B, d_C, M, N, K, alpha, beta);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float basicTime = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&basicTime, start, stop));\n    float basicGflops = 2.0 * M * N * K / basicTime / 1e6;\n\n    printf(\"Time: %.3f ms\\n\", basicTime);\n    printf(\"Performance: %.2f GFLOPS\\n\\n\", basicGflops);\n\n    // Copy result back\n    CUDA_CHECK(cudaMemcpy(h_C, d_C, bytesC, cudaMemcpyDeviceToHost));\n\n    // --- Test 2: Tiled WMMA GEMM ---\n    printf(\"Test 2: Tiled WMMA GEMM\\n\");\n\n    CUDA_CHECK(cudaMemset(d_C, 0, bytesC));\n\n    const int BLOCK_TILES = 4;\n    dim3 tiledBlockDim(128);\n    dim3 tiledGridDim((M + WMMA_M * BLOCK_TILES - 1) / (WMMA_M * BLOCK_TILES),\n                      (N + WMMA_N * BLOCK_TILES - 1) / (WMMA_N * BLOCK_TILES));\n\n    printf(\"Grid: (%d, %d), Block: %d\\n\", tiledGridDim.x, tiledGridDim.y, tiledBlockDim.x);\n\n    // Warm-up\n    wmmaGemmTiled<<<tiledGridDim, tiledBlockDim>>>(d_A, d_B, d_C, M, N, K, alpha, beta);\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    // Timed run\n    CUDA_CHECK(cudaEventRecord(start));\n    wmmaGemmTiled<<<tiledGridDim, tiledBlockDim>>>(d_A, d_B, d_C, M, N, K, alpha, beta);\n    CUDA_CHECK(cudaEventRecord(stop));\n    CUDA_CHECK(cudaDeviceSynchronize());\n\n    float tiledTime = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&tiledTime, start, stop));\n    float tiledGflops = 2.0 * M * N * K / tiledTime / 1e6;\n\n    printf(\"Time: %.3f ms\\n\", tiledTime);\n    printf(\"Performance: %.2f GFLOPS\\n\", tiledGflops);\n    printf(\"Speedup vs basic: %.2fx\\n\\n\", basicTime / tiledTime);\n\n    // Verify results\n    printf(\"Verifying results...\\n\");\n    cpuGemm(h_A, h_B, h_C_ref, M, N, K, alpha, beta);\n\n    float maxError = 0.0f;\n    for (int i = 0; i < M * N; i++) {\n        float error = fabsf(h_C[i] - h_C_ref[i]);\n        maxError = fmaxf(maxError, error);\n    }\n\n    printf(\"Max error: %f\\n\", maxError);\n    if (maxError < 1e-2) {\n        printf(\"Verification: PASSED\\n\\n\");\n    } else {\n        printf(\"Verification: FAILED\\n\\n\");\n    }\n\n    // Summary\n    printf(\"=== Performance Summary ===\\n\");\n    printf(\"Basic WMMA: %.2f GFLOPS\\n\", basicGflops);\n    printf(\"Tiled WMMA: %.2f GFLOPS\\n\\n\", tiledGflops);\n\n    printf(\"=== Key Takeaways ===\\n\");\n    printf(\"1. WMMA API provides direct access to Tensor Cores\\n\");\n    printf(\"2. Operations are performed on 16x16x16 matrix fragments\\n\");\n    printf(\"3. Mixed precision (FP16 input, FP32 accumulation) is standard\\n\");\n    printf(\"4. Tiling improves performance through better data reuse\\n\");\n    printf(\"5. For production, use cuBLAS which is highly optimized\\n\");\n\n    // Cleanup\n    CUDA_CHECK(cudaFree(d_A));\n    CUDA_CHECK(cudaFree(d_B));\n    CUDA_CHECK(cudaFree(d_C));\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n    free(h_A);\n    free(h_B);\n    free(h_C);\n    free(h_C_ref);\n    free(h_A_fp16);\n    free(h_B_fp16);\n\n    return 0;\n#endif\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Wmma Gemm ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. GEMM = General Matrix Multiply\n2. Highly optimized operation\n3. Tiling and register blocking\n4. cuBLAS provides production implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **56_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}