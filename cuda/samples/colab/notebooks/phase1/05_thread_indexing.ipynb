{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Advanced Thread Indexing\n",
    "## Phase 1: Foundations - Thread Hierarchy & Kernel Basics\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Master thread indexing in 1D, 2D, and 3D configurations\n",
    "- Understand grid-stride loops\n",
    "- Handle arbitrary data sizes efficiently\n",
    "- Learn best practices for thread indexing\n",
    "- Implement reusable indexing patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Thread Indexing Patterns\n",
    "\n",
    "**1D Indexing:**\n",
    "```cuda\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```\n",
    "\n",
    "**2D Indexing:**\n",
    "```cuda\n",
    "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "int idx = row * width + col;\n",
    "```\n",
    "\n",
    "**3D Indexing:**\n",
    "```cuda\n",
    "int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "int z = blockIdx.z * blockDim.z + threadIdx.z;\n",
    "int idx = z * width * height + y * width + x;\n",
    "```\n",
    "\n",
    "**Grid-Stride Loop:**\n",
    "```cuda\n",
    "int stride = blockDim.x * gridDim.x;\n",
    "for (int i = idx; i < n; i += stride) {\n",
    "    // Process element i\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Grid-Stride Loop for Large Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "__global__ void vectorAddGridStride(float *a, float *b, float *c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int stride = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Grid-stride loop handles any array size\n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = i * 1.0f;\n",
    "        h_b[i] = i * 2.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch with fewer blocks than elements\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = 1024;  // Fixed number of blocks\n",
    "    \n",
    "    printf(\"Array size: %d elements\\n\", n);\n",
    "    printf(\"Blocks: %d, Threads/block: %d\\n\", blocksPerGrid, threadsPerBlock);\n",
    "    printf(\"Total threads: %d\\n\", blocksPerGrid * threadsPerBlock);\n",
    "    printf(\"Each thread processes ~%d elements\\n\", n / (blocksPerGrid * threadsPerBlock));\n",
    "    \n",
    "    vectorAddGridStride<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "    \n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_c[i] != h_a[i] + h_b[i]) {\n",
    "            printf(\"Error at %d\\n\", i);\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"Result: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n",
    "    \n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: 3D Thread Indexing for Volume Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "__global__ void volumeAdd(float *a, float *b, float *c, int width, int height, int depth) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int z = blockIdx.z * blockDim.z + threadIdx.z;\n",
    "    \n",
    "    if (x < width && y < height && z < depth) {\n",
    "        int idx = z * (width * height) + y * width + x;\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "        \n",
    "        // Print info for first few elements\n",
    "        if (idx < 5) {\n",
    "            printf(\"Volume[%d][%d][%d] (idx=%d): Block(%d,%d,%d) Thread(%d,%d,%d)\\n\",\n",
    "                   z, y, x, idx,\n",
    "                   blockIdx.z, blockIdx.y, blockIdx.x,\n",
    "                   threadIdx.z, threadIdx.y, threadIdx.x);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int width = 32;\n",
    "    int height = 32;\n",
    "    int depth = 32;\n",
    "    int totalElements = width * height * depth;\n",
    "    size_t size = totalElements * sizeof(float);\n",
    "    \n",
    "    printf(\"Volume size: %dx%dx%d = %d elements\\n\", width, height, depth, totalElements);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < totalElements; i++) {\n",
    "        h_a[i] = i * 1.0f;\n",
    "        h_b[i] = i * 2.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // 3D configuration\n",
    "    dim3 blockDim(8, 8, 8);\n",
    "    dim3 gridDim((width + blockDim.x - 1) / blockDim.x,\n",
    "                 (height + blockDim.y - 1) / blockDim.y,\n",
    "                 (depth + blockDim.z - 1) / blockDim.z);\n",
    "    \n",
    "    printf(\"Block dimensions: %dx%dx%d\\n\", blockDim.x, blockDim.y, blockDim.z);\n",
    "    printf(\"Grid dimensions: %dx%dx%d\\n\\n\", gridDim.x, gridDim.y, gridDim.z);\n",
    "    \n",
    "    volumeAdd<<<gridDim, blockDim>>>(d_a, d_b, d_c, width, height, depth);\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < totalElements; i++) {\n",
    "        if (h_c[i] != h_a[i] + h_b[i]) {\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"\\nResult: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n",
    "    \n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Strided Access Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "// Process every Nth element\n",
    "__global__ void stridedAccess(float *input, float *output, int n, int stride) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Each thread processes elements at stride intervals\n",
    "    for (int i = idx; i < n; i += stride * blockDim.x * gridDim.x) {\n",
    "        output[i] = input[i] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "// Interleaved access pattern\n",
    "__global__ void interleavedAccess(float *input, float *output, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int totalThreads = blockDim.x * gridDim.x;\n",
    "    \n",
    "    // Each thread processes every Nth element\n",
    "    for (int i = idx; i < n; i += totalThreads) {\n",
    "        output[i] = input[i] * 2.0f;\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_input = (float*)malloc(size);\n",
    "    float *h_output = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_input[i] = i * 1.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_input, *d_output;\n",
    "    cudaMalloc(&d_input, size);\n",
    "    cudaMalloc(&d_output, size);\n",
    "    \n",
    "    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = 256;\n",
    "    \n",
    "    printf(\"Testing different access patterns:\\n\");\n",
    "    printf(\"Array size: %d elements\\n\\n\", n);\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    // Test strided access\n",
    "    cudaEventRecord(start);\n",
    "    stridedAccess<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n, 1);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float time1;\n",
    "    cudaEventElapsedTime(&time1, start, stop);\n",
    "    printf(\"Strided access: %.3f ms\\n\", time1);\n",
    "    \n",
    "    // Test interleaved access\n",
    "    cudaEventRecord(start);\n",
    "    interleavedAccess<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n",
    "    cudaEventRecord(stop);\n",
    "    cudaEventSynchronize(stop);\n",
    "    float time2;\n",
    "    cudaEventElapsedTime(&time2, start, stop);\n",
    "    printf(\"Interleaved access: %.3f ms\\n\", time2);\n",
    "    \n",
    "    cudaMemcpy(h_output, d_output, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_output[i] != h_input[i] * 2.0f) {\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"\\nResult: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n",
    "    \n",
    "    free(h_input); free(h_output);\n",
    "    cudaFree(d_input); cudaFree(d_output);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Thread ID Calculation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "// Utility functions for thread indexing\n",
    "__device__ int getGlobalIdx1D() {\n",
    "    return blockIdx.x * blockDim.x + threadIdx.x;\n",
    "}\n",
    "\n",
    "__device__ int getGlobalIdx2D(int width) {\n",
    "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    return row * width + col;\n",
    "}\n",
    "\n",
    "__device__ int getGlobalIdx3D(int width, int height) {\n",
    "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int z = blockIdx.z * blockDim.z + threadIdx.z;\n",
    "    return z * (width * height) + y * width + x;\n",
    "}\n",
    "\n",
    "__device__ int getTotalThreads1D() {\n",
    "    return blockDim.x * gridDim.x;\n",
    "}\n",
    "\n",
    "__device__ void printThreadInfo() {\n",
    "    printf(\"Thread info - Block: (%d,%d,%d), Thread: (%d,%d,%d), Global 1D: %d\\n\",\n",
    "           blockIdx.x, blockIdx.y, blockIdx.z,\n",
    "           threadIdx.x, threadIdx.y, threadIdx.z,\n",
    "           getGlobalIdx1D());\n",
    "}\n",
    "\n",
    "__global__ void demonstrateIndexing() {\n",
    "    int idx = getGlobalIdx1D();\n",
    "    \n",
    "    if (idx < 5) {\n",
    "        printThreadInfo();\n",
    "    }\n",
    "}\n",
    "\n",
    "__global__ void vectorAddWithUtilities(float *a, float *b, float *c, int n) {\n",
    "    int idx = getGlobalIdx1D();\n",
    "    int stride = getTotalThreads1D();\n",
    "    \n",
    "    for (int i = idx; i < n; i += stride) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Demonstrating indexing utilities:\\n\\n\");\n",
    "    \n",
    "    demonstrateIndexing<<<2, 4>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    // Test with vector addition\n",
    "    int n = 10000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = i * 1.0f;\n",
    "        h_b[i] = i * 2.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    vectorAddWithUtilities<<<256, 256>>>(d_a, d_b, d_c, n);\n",
    "    \n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_c[i] != h_a[i] + h_b[i]) {\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    printf(\"\\nVector addition with utilities: %s\\n\", correct ? \"CORRECT\" : \"INCORRECT\");\n",
    "    \n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise\n",
    "\n",
    "**Exercise 1:** Implement a kernel that processes every other element (even indices only)\n",
    "\n",
    "**Exercise 2:** Create a 3D kernel for image processing (RGB channels)\n",
    "\n",
    "**Exercise 3:** Implement a grid-stride loop for matrix multiplication\n",
    "\n",
    "**Exercise 4:** Write utility functions for converting between 1D and 2D/3D indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "// Your solution here\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void myKernel() {\n",
    "    // TODO: Implement your solution\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // TODO: Test your implementation\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Grid-stride loops** allow handling arrays larger than grid size\n",
    "2. **3D indexing** useful for volume data and multi-channel images\n",
    "3. **stride = blockDim.x * gridDim.x** for 1D grid-stride\n",
    "4. Always **bounds check** with all indexing patterns\n",
    "5. **Utility functions** improve code readability and reusability\n",
    "6. Different access patterns have different performance characteristics\n",
    "7. Choose indexing pattern based on data structure and access pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Congratulations on completing Phase 1! You now understand:\n",
    "- CUDA basics and GPU architecture\n",
    "- Thread hierarchy and kernel launches\n",
    "- Memory management and data transfer\n",
    "- Thread indexing patterns\n",
    "\n",
    "In Phase 2, we'll dive into:\n",
    "- Memory types and optimization\n",
    "- Shared memory usage\n",
    "- Memory coalescing\n",
    "- Bandwidth optimization\n",
    "\n",
    "Continue to: **Phase 2 - 06_memory_basics.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "*Use this space to write your own notes and observations:*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
