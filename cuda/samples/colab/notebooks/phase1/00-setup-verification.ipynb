{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Setup Verification\n",
    "\n",
    "## Learning Objectives\n",
    "- Verify CUDA is available in Colab\n",
    "- Check GPU specifications\n",
    "- Understand compute capability\n",
    "- Run your first CUDA code\n",
    "\n",
    "## Before You Start\n",
    "**IMPORTANT**: Make sure you've enabled GPU runtime\n",
    "- Click `Runtime` \u2192 `Change runtime type`\n",
    "- Select `T4 GPU` from Hardware accelerator\n",
    "- Click `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check NVIDIA Driver and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== 00-Setup-Verification ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Look For:\n",
    "- GPU name (e.g., Tesla T4, V100)\n",
    "- Driver version\n",
    "- CUDA version\n",
    "- GPU memory (e.g., 15GB for T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check CUDA Compiler Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== 00-Setup-Verification ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install CUDA Extension for Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nvcc plugin for Jupyter notebooks\n",
    "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== 00-Setup-Verification ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Your First CUDA Program - Hello World!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void helloFromGPU() {\n",
    "    printf(\"Hello from GPU thread %d!\\n\", threadIdx.x);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Hello from CPU!\\n\");\n",
    "    \n",
    "    // Launch kernel with 1 block and 10 threads\n",
    "    helloFromGPU<<<1, 10>>>();\n",
    "    \n",
    "    // Wait for GPU to finish\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    printf(\"Back to CPU!\\n\");\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Code\n",
    "\n",
    "- `__global__`: Indicates this function runs on GPU and is called from CPU\n",
    "- `<<<1, 10>>>`: Launch configuration (1 block, 10 threads)\n",
    "- `threadIdx.x`: Built-in variable for thread index\n",
    "- `cudaDeviceSynchronize()`: Wait for GPU to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Query GPU Properties in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "    int deviceCount;\n",
    "    cudaGetDeviceCount(&deviceCount);\n",
    "    \n",
    "    printf(\"Number of CUDA devices: %d\\n\\n\", deviceCount);\n",
    "    \n",
    "    for (int i = 0; i < deviceCount; i++) {\n",
    "        cudaDeviceProp prop;\n",
    "        cudaGetDeviceProperties(&prop, i);\n",
    "        \n",
    "        printf(\"Device %d: %s\\n\", i, prop.name);\n",
    "        printf(\"========================================\\n\");\n",
    "        printf(\"Compute Capability: %d.%d\\n\", prop.major, prop.minor);\n",
    "        printf(\"Total Global Memory: %.2f GB\\n\", prop.totalGlobalMem / 1e9);\n",
    "        printf(\"Shared Memory per Block: %.2f KB\\n\", prop.sharedMemPerBlock / 1024.0);\n",
    "        printf(\"Registers per Block: %d\\n\", prop.regsPerBlock);\n",
    "        printf(\"Warp Size: %d\\n\", prop.warpSize);\n",
    "        printf(\"Max Threads per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
    "        printf(\"Max Threads Dimensions: (%d, %d, %d)\\n\", \n",
    "               prop.maxThreadsDim[0], prop.maxThreadsDim[1], prop.maxThreadsDim[2]);\n",
    "        printf(\"Max Grid Dimensions: (%d, %d, %d)\\n\",\n",
    "               prop.maxGridSize[0], prop.maxGridSize[1], prop.maxGridSize[2]);\n",
    "        printf(\"Number of Multiprocessors: %d\\n\", prop.multiProcessorCount);\n",
    "        printf(\"Clock Rate: %.2f GHz\\n\", prop.clockRate / 1e6);\n",
    "        printf(\"Memory Clock Rate: %.2f GHz\\n\", prop.memoryClockRate / 1e6);\n",
    "        printf(\"Memory Bus Width: %d bits\\n\", prop.memoryBusWidth);\n",
    "        printf(\"L2 Cache Size: %.2f MB\\n\", prop.l2CacheSize / 1e6);\n",
    "        printf(\"Max Threads per Multiprocessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Properties to Remember\n",
    "\n",
    "- **Compute Capability**: Determines which CUDA features are available\n",
    "- **Multiprocessors (SMs)**: Number of parallel processing units\n",
    "- **Max Threads per Block**: Usually 1024\n",
    "- **Warp Size**: Always 32 (threads execute in warps)\n",
    "- **Shared Memory**: Fast on-chip memory shared by threads in a block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Understanding Thread Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void printThreadInfo() {\n",
    "    int blockId = blockIdx.x;\n",
    "    int threadId = threadIdx.x;\n",
    "    int globalId = blockId * blockDim.x + threadId;\n",
    "    \n",
    "    printf(\"Block %d, Thread %d, Global ID %d\\n\", blockId, threadId, globalId);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching kernel with 3 blocks and 4 threads per block\\n\");\n",
    "    printf(\"Total threads: 3 * 4 = 12\\n\\n\");\n",
    "    \n",
    "    printThreadInfo<<<3, 4>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread Hierarchy Explained\n",
    "\n",
    "```\n",
    "Grid (All blocks)\n",
    "  \u251c\u2500 Block 0\n",
    "  \u2502   \u251c\u2500 Thread 0 (Global ID: 0)\n",
    "  \u2502   \u251c\u2500 Thread 1 (Global ID: 1)\n",
    "  \u2502   \u251c\u2500 Thread 2 (Global ID: 2)\n",
    "  \u2502   \u2514\u2500 Thread 3 (Global ID: 3)\n",
    "  \u251c\u2500 Block 1\n",
    "  \u2502   \u251c\u2500 Thread 0 (Global ID: 4)\n",
    "  \u2502   \u251c\u2500 Thread 1 (Global ID: 5)\n",
    "  \u2502   \u251c\u2500 Thread 2 (Global ID: 6)\n",
    "  \u2502   \u2514\u2500 Thread 3 (Global ID: 7)\n",
    "  \u2514\u2500 Block 2\n",
    "      \u251c\u2500 Thread 0 (Global ID: 8)\n",
    "      \u251c\u2500 Thread 1 (Global ID: 9)\n",
    "      \u251c\u2500 Thread 2 (Global ID: 10)\n",
    "      \u2514\u2500 Thread 3 (Global ID: 11)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Error Checking (Important!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "#define CHECK_CUDA_ERROR(call) { \\\n",
    "    cudaError_t err = call; \\\n",
    "    if (err != cudaSuccess) { \\\n",
    "        printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "               cudaGetErrorString(err)); \\\n",
    "        exit(1); \\\n",
    "    } \\\n",
    "}\n",
    "\n",
    "__global__ void kernel() {\n",
    "    printf(\"Hello from kernel!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Example 1: Check memory allocation\n",
    "    float *d_data;\n",
    "    CHECK_CUDA_ERROR(cudaMalloc(&d_data, 1000 * sizeof(float)));\n",
    "    printf(\"Memory allocated successfully\\n\");\n",
    "    \n",
    "    // Example 2: Check kernel launch\n",
    "    kernel<<<1, 32>>>();\n",
    "    CHECK_CUDA_ERROR(cudaGetLastError());\n",
    "    CHECK_CUDA_ERROR(cudaDeviceSynchronize());\n",
    "    printf(\"Kernel executed successfully\\n\");\n",
    "    \n",
    "    // Clean up\n",
    "    CHECK_CUDA_ERROR(cudaFree(d_data));\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 Checklist: You're Ready to Proceed If...\n",
    "\n",
    "- [ ] You can see GPU information with `nvidia-smi`\n",
    "- [ ] You successfully ran the Hello World kernel\n",
    "- [ ] You understand the thread hierarchy (blocks and threads)\n",
    "- [ ] You can query GPU properties\n",
    "- [ ] You know how to check for CUDA errors\n",
    "\n",
    "## \ud83c\udf89 Congratulations!\n",
    "\n",
    "You've successfully set up your CUDA learning environment and run your first GPU code!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Continue to:\n",
    "- `01-hello-world.ipynb` - More kernel launch patterns\n",
    "- `02-device-query.ipynb` - Deep dive into GPU architecture\n",
    "- `03-thread-indexing.ipynb` - Master thread indexing\n",
    "\n",
    "## Notes Section (Your Learning Journal)\n",
    "\n",
    "Use this space to write notes about what you learned:\n",
    "\n",
    "---\n",
    "\n",
    "*Add your notes here*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}