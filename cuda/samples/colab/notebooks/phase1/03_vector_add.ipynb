{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Vector Addition on GPU\n",
    "## Phase 1: Foundations - Thread Hierarchy & Kernel Basics\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand memory allocation and deallocation on GPU\n",
    "- Learn data transfer between host and device\n",
    "- Implement parallel vector addition\n",
    "- Calculate global thread indices correctly\n",
    "- Compare CPU vs GPU performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Memory Management and Data Transfer\n",
    "\n",
    "**CUDA Memory Management Functions:**\n",
    "- `cudaMalloc()` - Allocate memory on GPU\n",
    "- `cudaFree()` - Free GPU memory\n",
    "- `cudaMemcpy()` - Copy data between host and device\n",
    "\n",
    "**Memory Transfer Directions:**\n",
    "- `cudaMemcpyHostToDevice` - CPU → GPU\n",
    "- `cudaMemcpyDeviceToHost` - GPU → CPU\n",
    "- `cudaMemcpyDeviceToDevice` - GPU → GPU\n",
    "\n",
    "**Calculating Global Thread Index:**\n",
    "```cuda\n",
    "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Vector Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    // Calculate global thread index\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // Boundary check\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    // Allocate host memory\n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    // Initialize input vectors\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = i * 1.0f;\n",
    "        h_b[i] = i * 2.0f;\n",
    "    }\n",
    "    \n",
    "    // Allocate device memory\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    // Copy data to device\n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Launch kernel\n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    printf(\"Launching kernel with %d blocks and %d threads per block\\n\",\n",
    "           blocksPerGrid, threadsPerBlock);\n",
    "    \n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "    \n",
    "    // Copy result back to host\n",
    "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    // Verify result\n",
    "    bool correct = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_c[i] != h_a[i] + h_b[i]) {\n",
    "            printf(\"Error at index %d: %f != %f\\n\", i, h_c[i], h_a[i] + h_b[i]);\n",
    "            correct = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if (correct) {\n",
    "        printf(\"Vector addition successful!\\n\");\n",
    "        printf(\"Sample results: %f + %f = %f\\n\", h_a[0], h_b[0], h_c[0]);\n",
    "    }\n",
    "    \n",
    "    // Free memory\n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Vector Addition with Error Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = rand() / (float)RAND_MAX;\n",
    "        h_b[i] = rand() / (float)RAND_MAX;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    CUDA_CHECK(cudaMalloc(&d_a, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_b, size));\n",
    "    CUDA_CHECK(cudaMalloc(&d_c, size));\n",
    "    \n",
    "    CUDA_CHECK(cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice));\n",
    "    CUDA_CHECK(cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice));\n",
    "    \n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    CUDA_CHECK(cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost));\n",
    "    \n",
    "    // Verify\n",
    "    float maxError = 0.0f;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        float error = abs(h_c[i] - (h_a[i] + h_b[i]));\n",
    "        if (error > maxError) maxError = error;\n",
    "    }\n",
    "    printf(\"Max error: %f\\n\", maxError);\n",
    "    printf(\"Vector addition completed successfully!\\n\");\n",
    "    \n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: CPU vs GPU Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "__global__ void vectorAddGPU(float *a, float *b, float *c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "void vectorAddCPU(float *a, float *b, float *c, int n) {\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 10000000;  // 10 million elements\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    printf(\"Vector size: %d elements (%.2f MB)\\n\", n, size / 1024.0 / 1024.0);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c_cpu = (float*)malloc(size);\n",
    "    float *h_c_gpu = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = i * 1.0f;\n",
    "        h_b[i] = i * 2.0f;\n",
    "    }\n",
    "    \n",
    "    // CPU timing\n",
    "    clock_t start_cpu = clock();\n",
    "    vectorAddCPU(h_a, h_b, h_c_cpu, n);\n",
    "    clock_t end_cpu = clock();\n",
    "    double cpu_time = ((double)(end_cpu - start_cpu)) / CLOCKS_PER_SEC * 1000.0;\n",
    "    \n",
    "    // GPU timing\n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaEvent_t start_gpu, stop_gpu;\n",
    "    cudaEventCreate(&start_gpu);\n",
    "    cudaEventCreate(&stop_gpu);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    int threadsPerBlock = 256;\n",
    "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "    \n",
    "    cudaEventRecord(start_gpu);\n",
    "    vectorAddGPU<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "    cudaEventRecord(stop_gpu);\n",
    "    \n",
    "    cudaMemcpy(h_c_gpu, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    \n",
    "    cudaEventSynchronize(stop_gpu);\n",
    "    float gpu_time = 0;\n",
    "    cudaEventElapsedTime(&gpu_time, start_gpu, stop_gpu);\n",
    "    \n",
    "    // Verify results match\n",
    "    bool match = true;\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        if (h_c_cpu[i] != h_c_gpu[i]) {\n",
    "            match = false;\n",
    "            break;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    printf(\"\\nResults:\\n\");\n",
    "    printf(\"CPU Time: %.2f ms\\n\", cpu_time);\n",
    "    printf(\"GPU Time: %.2f ms\\n\", gpu_time);\n",
    "    printf(\"Speedup: %.2fx\\n\", cpu_time / gpu_time);\n",
    "    printf(\"Results match: %s\\n\", match ? \"YES\" : \"NO\");\n",
    "    \n",
    "    free(h_a); free(h_b); free(h_c_cpu); free(h_c_gpu);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    cudaEventDestroy(start_gpu);\n",
    "    cudaEventDestroy(stop_gpu);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Different Thread Block Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "__global__ void vectorAdd(float *a, float *b, float *c, int n) {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx < n) {\n",
    "        c[idx] = a[idx] + b[idx];\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int n = 1000000;\n",
    "    size_t size = n * sizeof(float);\n",
    "    \n",
    "    float *h_a = (float*)malloc(size);\n",
    "    float *h_b = (float*)malloc(size);\n",
    "    float *h_c = (float*)malloc(size);\n",
    "    \n",
    "    for (int i = 0; i < n; i++) {\n",
    "        h_a[i] = i * 1.0f;\n",
    "        h_b[i] = i * 2.0f;\n",
    "    }\n",
    "    \n",
    "    float *d_a, *d_b, *d_c;\n",
    "    cudaMalloc(&d_a, size);\n",
    "    cudaMalloc(&d_b, size);\n",
    "    cudaMalloc(&d_c, size);\n",
    "    \n",
    "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
    "    \n",
    "    // Test different block sizes\n",
    "    int blockSizes[] = {32, 64, 128, 256, 512, 1024};\n",
    "    \n",
    "    cudaEvent_t start, stop;\n",
    "    cudaEventCreate(&start);\n",
    "    cudaEventCreate(&stop);\n",
    "    \n",
    "    printf(\"Testing different block sizes:\\n\");\n",
    "    printf(\"----------------------------------------\\n\");\n",
    "    \n",
    "    for (int i = 0; i < 6; i++) {\n",
    "        int threadsPerBlock = blockSizes[i];\n",
    "        int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
    "        \n",
    "        cudaEventRecord(start);\n",
    "        vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);\n",
    "        cudaEventRecord(stop);\n",
    "        cudaEventSynchronize(stop);\n",
    "        \n",
    "        float milliseconds = 0;\n",
    "        cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "        \n",
    "        printf(\"Block size %4d: %d blocks, Time: %.3f ms\\n\",\n",
    "               threadsPerBlock, blocksPerGrid, milliseconds);\n",
    "    }\n",
    "    \n",
    "    free(h_a); free(h_b); free(h_c);\n",
    "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
    "    cudaEventDestroy(start);\n",
    "    cudaEventDestroy(stop);\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise\n",
    "\n",
    "**Exercise 1:** Implement vector subtraction (c[i] = a[i] - b[i])\n",
    "\n",
    "**Exercise 2:** Implement vector scaling (c[i] = a[i] * scalar)\n",
    "\n",
    "**Exercise 3:** Implement element-wise vector multiplication (c[i] = a[i] * b[i])\n",
    "\n",
    "**Exercise 4:** Test with different data sizes and find the break-even point where GPU becomes faster than CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "// Your solution here\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "__global__ void vectorOp(float *a, float *b, float *c, int n) {\n",
    "    // TODO: Implement your operation\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // TODO: Implement your solution\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Global thread index** calculation: `blockIdx.x * blockDim.x + threadIdx.x`\n",
    "2. Always perform **boundary checks** to avoid memory access errors\n",
    "3. **cudaMalloc** allocates GPU memory, **cudaFree** releases it\n",
    "4. **cudaMemcpy** transfers data between host and device\n",
    "5. Calculate blocks as: `(n + threadsPerBlock - 1) / threadsPerBlock`\n",
    "6. GPU shows speedup for sufficiently large data sizes\n",
    "7. Different block sizes can affect performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll learn how to:\n",
    "- Work with 2D thread blocks and grids\n",
    "- Perform matrix addition\n",
    "- Calculate 2D thread indices\n",
    "- Handle 2D data structures on GPU\n",
    "\n",
    "Continue to: **04_matrix_add.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "*Use this space to write your own notes and observations:*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
