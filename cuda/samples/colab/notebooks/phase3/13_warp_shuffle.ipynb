{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 13: Warp-Level Primitives and Shuffle Operations",
    "## Phase 3: Optimization Fundamentals",
    "",
    "**Learning Objectives:**",
    "- Understand __shfl operations",
    "- Learn warp-level communication",
    "- Master reduction",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Warp-Level Primitives and Shuffle Operations",
    "",
    "**Topics Covered:**",
    "- __shfl operations",
    "- warp-level communication",
    "- reduction",
    "",
    "**Key Concepts:**",
    "This notebook covers __shfl operations in the context of Phase 3: Optimization Fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Warp-Level Primitives and Shuffle Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu",
    "#include <stdio.h>",
    "#include <cuda_runtime.h>",
    "",
    "#define CUDA_CHECK(call) \\",
    "    do { \\",
    "        cudaError_t err = call; \\",
    "        if (err != cudaSuccess) { \\",
    "            fprintf(stderr, \"CUDA error: %s\\n\", cudaGetErrorString(err)); \\",
    "            exit(EXIT_FAILURE); \\",
    "        } \\",
    "    } while(0)",
    "",
    "__global__ void warpReduceKernel(float *input, float *output, int n) {",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;",
    "    int lane = threadIdx.x % 32;",
    "    int warpId = threadIdx.x / 32;",
    "",
    "    float val = (idx < n) ? input[idx] : 0.0f;",
    "",
    "    for (int offset = 16; offset > 0; offset /= 2) {",
    "        val += __shfl_down_sync(0xffffffff, val, offset);",
    "    }",
    "",
    "    if (lane == 0) {",
    "        output[blockIdx.x * (blockDim.x / 32) + warpId] = val;",
    "    }",
    "}",
    "",
    "int main() {",
    "    printf(\"=== Warp Shuffle ===\\n\\n\");",
    "    int n = 1 << 20;",
    "    float *d_input, *d_output;",
    "    CUDA_CHECK(cudaMalloc(&d_input, n * sizeof(float)));",
    "    CUDA_CHECK(cudaMalloc(&d_output, (n/32) * sizeof(float)));",
    "",
    "    int threads = 256, blocks = (n + threads - 1) / threads;",
    "    cudaEvent_t start, stop;",
    "    cudaEventCreate(&start);",
    "    cudaEventCreate(&stop);",
    "",
    "    cudaEventRecord(start);",
    "    warpReduceKernel<<<blocks, threads>>>(d_input, d_output, n);",
    "    cudaEventRecord(stop);",
    "    cudaEventSynchronize(stop);",
    "",
    "    float ms;",
    "    cudaEventElapsedTime(&ms, start, stop);",
    "    printf(\"Time: %.2f ms\\n\", ms);",
    "",
    "    cudaFree(d_input);",
    "    cudaFree(d_output);",
    "    cudaEventDestroy(start);",
    "    cudaEventDestroy(stop);",
    "    return 0;",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n__global__ void reductionOptimized(float *input, float *output, int n) {\n    __shared__ float sdata[256];\n\n    int tid = threadIdx.x;\n    int idx = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n\n    // Load data with grid-stride and add during load\n    sdata[tid] = 0;\n    if (idx < n) sdata[tid] = input[idx];\n    if (idx + blockDim.x < n) sdata[tid] += input[idx + blockDim.x];\n    __syncthreads();\n\n    // Sequential addressing (no divergence)\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        output[blockIdx.x] = sdata[0];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    printf(\"=== Optimized Parallel Reduction ===\\n\\n\");\n\n    float *h_input = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_input[i] = 1.0f;\n\n    float *d_input, *d_output;\n    cudaMalloc(&d_input, size);\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (n + threadsPerBlock * 2 - 1) / (threadsPerBlock * 2);\n    cudaMalloc(&d_output, blocksPerGrid * sizeof(float));\n\n    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    cudaEventRecord(start);\n    reductionOptimized<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float milliseconds = 0;\n    cudaEventElapsedTime(&milliseconds, start, stop);\n\n    float *h_output = (float*)malloc(blocksPerGrid * sizeof(float));\n    cudaMemcpy(h_output, d_output, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost);\n\n    float sum = 0;\n    for (int i = 0; i < blocksPerGrid; i++) {\n        sum += h_output[i];\n    }\n\n    printf(\"Sum: %.0f (expected: %d)\\n\", sum, n);\n    printf(\"Time: %.3f ms\\n\", milliseconds);\n    printf(\"Result: %s\\n\\n\", (sum == n) ? \"CORRECT\" : \"INCORRECT\");\n    printf(\"OPTIMIZATION: Sequential addressing avoids warp divergence!\\n\");\n\n    free(h_input); free(h_output);\n    cudaFree(d_input); cudaFree(d_output);\n    cudaEventDestroy(start); cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Warp shuffle allows intra-warp communication\n2. No shared memory required\n3. Lower latency than shared memory\n4. Operations: __shfl_sync, __shfl_down_sync, __shfl_up_sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **14_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}