{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 15: Parallel Reduction Algorithms",
    "## Phase 3: Optimization Fundamentals",
    "",
    "**Learning Objectives:**",
    "- Understand reduction tree",
    "- Learn shared memory reduction",
    "- Master warp reduction",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: Parallel Reduction Algorithms",
    "",
    "**Topics Covered:**",
    "- reduction tree",
    "- shared memory reduction",
    "- warp reduction",
    "",
    "**Key Concepts:**",
    "This notebook covers reduction tree in the context of Phase 3: Optimization Fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic Parallel Reduction Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n__global__ void reductionNaive(float *input, float *output, int n) {\n    __shared__ float sdata[256];\n\n    int tid = threadIdx.x;\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Load data into shared memory\n    sdata[tid] = (idx < n) ? input[idx] : 0.0f;\n    __syncthreads();\n\n    // Reduction in shared memory (naive approach)\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0 && tid + s < blockDim.x) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    // Write result for this block\n    if (tid == 0) {\n        output[blockIdx.x] = sdata[0];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    printf(\"=== Naive Parallel Reduction ===\\n\\n\");\n\n    float *h_input = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_input[i] = 1.0f;  // Sum should be n\n\n    float *d_input, *d_output;\n    cudaMalloc(&d_input, size);\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n    cudaMalloc(&d_output, blocksPerGrid * sizeof(float));\n\n    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n\n    // Launch kernel\n    reductionNaive<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n\n    // Copy partial results and finish on CPU\n    float *h_output = (float*)malloc(blocksPerGrid * sizeof(float));\n    cudaMemcpy(h_output, d_output, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost);\n\n    float sum = 0;\n    for (int i = 0; i < blocksPerGrid; i++) {\n        sum += h_output[i];\n    }\n\n    printf(\"Sum: %.0f (expected: %d)\\n\", sum, n);\n    printf(\"Result: %s\\n\\n\", (sum == n) ? \"CORRECT\" : \"INCORRECT\");\n    printf(\"NOTE: This naive version has divergent branches (inefficient)\\n\");\n\n    free(h_input); free(h_output);\n    cudaFree(d_input); cudaFree(d_output);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n\n__global__ void reductionOptimized(float *input, float *output, int n) {\n    __shared__ float sdata[256];\n\n    int tid = threadIdx.x;\n    int idx = blockIdx.x * (blockDim.x * 2) + threadIdx.x;\n\n    // Load data with grid-stride and add during load\n    sdata[tid] = 0;\n    if (idx < n) sdata[tid] = input[idx];\n    if (idx + blockDim.x < n) sdata[tid] += input[idx + blockDim.x];\n    __syncthreads();\n\n    // Sequential addressing (no divergence)\n    for (int s = blockDim.x / 2; s > 0; s >>= 1) {\n        if (tid < s) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        output[blockIdx.x] = sdata[0];\n    }\n}\n\nint main() {\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    printf(\"=== Optimized Parallel Reduction ===\\n\\n\");\n\n    float *h_input = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_input[i] = 1.0f;\n\n    float *d_input, *d_output;\n    cudaMalloc(&d_input, size);\n\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (n + threadsPerBlock * 2 - 1) / (threadsPerBlock * 2);\n    cudaMalloc(&d_output, blocksPerGrid * sizeof(float));\n\n    cudaMemcpy(d_input, h_input, size, cudaMemcpyHostToDevice);\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    cudaEventRecord(start);\n    reductionOptimized<<<blocksPerGrid, threadsPerBlock>>>(d_input, d_output, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float milliseconds = 0;\n    cudaEventElapsedTime(&milliseconds, start, stop);\n\n    float *h_output = (float*)malloc(blocksPerGrid * sizeof(float));\n    cudaMemcpy(h_output, d_output, blocksPerGrid * sizeof(float), cudaMemcpyDeviceToHost);\n\n    float sum = 0;\n    for (int i = 0; i < blocksPerGrid; i++) {\n        sum += h_output[i];\n    }\n\n    printf(\"Sum: %.0f (expected: %d)\\n\", sum, n);\n    printf(\"Time: %.3f ms\\n\", milliseconds);\n    printf(\"Result: %s\\n\\n\", (sum == n) ? \"CORRECT\" : \"INCORRECT\");\n    printf(\"OPTIMIZATION: Sequential addressing avoids warp divergence!\\n\");\n\n    free(h_input); free(h_output);\n    cudaFree(d_input); cudaFree(d_output);\n    cudaEventDestroy(start); cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. Reduction combines array elements (sum, max, min, etc.)\n2. Requires sequential addressing in shared memory\n3. Avoid divergent warps for better performance\n4. Multiple optimization levels possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **16_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}