{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 51: CUDA Graphs",
    "## Phase 9: Advanced Topics",
    "",
    "**Learning Objectives:**",
    "- Understand CUDA graphs",
    "- Learn graph capture",
    "- Master optimization",
    "- Apply concepts in practical scenarios",
    "- Measure and analyze performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: CUDA Graphs",
    "",
    "**Topics Covered:**",
    "- CUDA graphs",
    "- graph capture",
    "- optimization",
    "",
    "**Key Concepts:**",
    "This notebook covers CUDA graphs in the context of Phase 9: Advanced Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Basic CUDA Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%cu\n\n/*\n * CUDA Graphs - Capture and replay kernel sequences\n *\n * CUDA Graphs allow you to define a workflow once and replay it multiple\n * times with reduced overhead. This is particularly useful for repeated\n * kernel launches with the same structure.\n *\n * Benefits:\n * - Reduced CPU overhead\n * - Better optimization opportunities\n * - Simplified code for repeated operations\n */\n\n#include <stdio.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\\n                    cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n// Kernel 1: Initialize array\n__global__ void initKernel(float *data, int n, float value) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = value;\n    }\n}\n\n// Kernel 2: Scale array by factor\n__global__ void scaleKernel(float *data, int n, float scale) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] *= scale;\n    }\n}\n\n// Kernel 3: Add offset to array\n__global__ void addKernel(float *data, int n, float offset) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] += offset;\n    }\n}\n\n// Kernel 4: Compute square\n__global__ void squareKernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * data[idx];\n    }\n}\n\nint main() {\n    printf(\"=== CUDA Graphs Demo ===\\n\\n\");\n\n    // Problem size\n    const int N = 1024 * 1024;  // 1M elements\n    const size_t bytes = N * sizeof(float);\n\n    // Allocate device memory\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, bytes));\n\n    // Create stream\n    cudaStream_t stream;\n    CUDA_CHECK(cudaStreamCreate(&stream));\n\n    // Create events for timing\n    cudaEvent_t start, stop;\n    CUDA_CHECK(cudaEventCreate(&start));\n    CUDA_CHECK(cudaEventCreate(&stop));\n\n    // Kernel launch parameters\n    int threadsPerBlock = 256;\n    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;\n\n    printf(\"Array size: %d elements\\n\", N);\n    printf(\"Grid: %d blocks, Block: %d threads\\n\\n\", blocksPerGrid, threadsPerBlock);\n\n    // --- Test 1: Traditional kernel launches ---\n    printf(\"Test 1: Traditional Kernel Launches\\n\");\n    printf(\"Running 100 iterations...\\n\");\n\n    CUDA_CHECK(cudaEventRecord(start, stream));\n\n    for (int i = 0; i < 100; i++) {\n        initKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N, 1.0f);\n        scaleKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N, 2.0f);\n        addKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N, 3.0f);\n        squareKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N);\n    }\n\n    CUDA_CHECK(cudaEventRecord(stop, stream));\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n\n    float traditionalTime = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&traditionalTime, start, stop));\n    printf(\"Total time: %.3f ms\\n\", traditionalTime);\n    printf(\"Average per iteration: %.3f ms\\n\\n\", traditionalTime / 100);\n\n    // --- Test 2: CUDA Graph - Stream Capture ---\n    printf(\"Test 2: CUDA Graph (Stream Capture)\\n\");\n\n    cudaGraph_t graph;\n    cudaGraphExec_t graphExec;\n\n    // Capture the sequence of operations\n    printf(\"Capturing graph...\\n\");\n    CUDA_CHECK(cudaStreamBeginCapture(stream, cudaStreamCaptureModeGlobal));\n\n    initKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N, 1.0f);\n    scaleKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N, 2.0f);\n    addKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N, 3.0f);\n    squareKernel<<<blocksPerGrid, threadsPerBlock, 0, stream>>>(d_data, N);\n\n    CUDA_CHECK(cudaStreamEndCapture(stream, &graph));\n\n    // Instantiate the graph\n    CUDA_CHECK(cudaGraphInstantiate(&graphExec, graph, NULL, NULL, 0));\n\n    printf(\"Running 100 iterations...\\n\");\n\n    CUDA_CHECK(cudaEventRecord(start, stream));\n\n    for (int i = 0; i < 100; i++) {\n        CUDA_CHECK(cudaGraphLaunch(graphExec, stream));\n    }\n\n    CUDA_CHECK(cudaEventRecord(stop, stream));\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n\n    float graphTime = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&graphTime, start, stop));\n    printf(\"Total time: %.3f ms\\n\", graphTime);\n    printf(\"Average per iteration: %.3f ms\\n\\n\", graphTime / 100);\n\n    // --- Test 3: CUDA Graph - Manual Construction ---\n    printf(\"Test 3: CUDA Graph (Manual Construction)\\n\");\n\n    cudaGraph_t manualGraph;\n    cudaGraphExec_t manualGraphExec;\n\n    CUDA_CHECK(cudaGraphCreate(&manualGraph, 0));\n\n    // Create nodes manually\n    cudaGraphNode_t initNode, scaleNode, addNode, squareNode;\n    cudaKernelNodeParams initParams = {0};\n    cudaKernelNodeParams scaleParams = {0};\n    cudaKernelNodeParams addParams = {0};\n    cudaKernelNodeParams squareParams = {0};\n\n    // Setup init kernel parameters\n    void *initArgs[] = {&d_data, &N, &(float){1.0f}};\n    initParams.func = (void*)initKernel;\n    initParams.gridDim = dim3(blocksPerGrid);\n    initParams.blockDim = dim3(threadsPerBlock);\n    initParams.kernelParams = initArgs;\n\n    // Setup scale kernel parameters\n    void *scaleArgs[] = {&d_data, &N, &(float){2.0f}};\n    scaleParams.func = (void*)scaleKernel;\n    scaleParams.gridDim = dim3(blocksPerGrid);\n    scaleParams.blockDim = dim3(threadsPerBlock);\n    scaleParams.kernelParams = scaleArgs;\n\n    // Setup add kernel parameters\n    void *addArgs[] = {&d_data, &N, &(float){3.0f}};\n    addParams.func = (void*)addKernel;\n    addParams.gridDim = dim3(blocksPerGrid);\n    addParams.blockDim = dim3(threadsPerBlock);\n    addParams.kernelParams = addArgs;\n\n    // Setup square kernel parameters\n    void *squareArgs[] = {&d_data, &N};\n    squareParams.func = (void*)squareKernel;\n    squareParams.gridDim = dim3(blocksPerGrid);\n    squareParams.blockDim = dim3(threadsPerBlock);\n    squareParams.kernelParams = squareArgs;\n\n    // Add nodes to graph with dependencies\n    CUDA_CHECK(cudaGraphAddKernelNode(&initNode, manualGraph, NULL, 0, &initParams));\n    CUDA_CHECK(cudaGraphAddKernelNode(&scaleNode, manualGraph, &initNode, 1, &scaleParams));\n    CUDA_CHECK(cudaGraphAddKernelNode(&addNode, manualGraph, &scaleNode, 1, &addParams));\n    CUDA_CHECK(cudaGraphAddKernelNode(&squareNode, manualGraph, &addNode, 1, &squareParams));\n\n    // Instantiate the manually constructed graph\n    CUDA_CHECK(cudaGraphInstantiate(&manualGraphExec, manualGraph, NULL, NULL, 0));\n\n    printf(\"Running 100 iterations...\\n\");\n\n    CUDA_CHECK(cudaEventRecord(start, stream));\n\n    for (int i = 0; i < 100; i++) {\n        CUDA_CHECK(cudaGraphLaunch(manualGraphExec, stream));\n    }\n\n    CUDA_CHECK(cudaEventRecord(stop, stream));\n    CUDA_CHECK(cudaStreamSynchronize(stream));\n\n    float manualGraphTime = 0;\n    CUDA_CHECK(cudaEventElapsedTime(&manualGraphTime, start, stop));\n    printf(\"Total time: %.3f ms\\n\", manualGraphTime);\n    printf(\"Average per iteration: %.3f ms\\n\\n\", manualGraphTime / 100);\n\n    // Performance comparison\n    printf(\"=== Performance Summary ===\\n\");\n    printf(\"Traditional launches: %.3f ms/iter\\n\", traditionalTime / 100);\n    printf(\"Stream capture graph: %.3f ms/iter (%.1fx speedup)\\n\",\n           graphTime / 100, traditionalTime / graphTime);\n    printf(\"Manual graph:         %.3f ms/iter (%.1fx speedup)\\n\\n\",\n           manualGraphTime / 100, traditionalTime / manualGraphTime);\n\n    // Verify final result\n    float *h_result = (float*)malloc(bytes);\n    CUDA_CHECK(cudaMemcpy(h_result, d_data, bytes, cudaMemcpyDeviceToHost));\n\n    // Expected: ((1.0 * 2.0) + 3.0)^2 = 5.0^2 = 25.0\n    float expected = 25.0f;\n    bool correct = true;\n    for (int i = 0; i < 10; i++) {\n        if (fabsf(h_result[i] - expected) > 1e-5) {\n            printf(\"Verification failed at %d: got %f, expected %f\\n\", i, h_result[i], expected);\n            correct = false;\n            break;\n        }\n    }\n\n    if (correct) {\n        printf(\"Verification: PASSED\\n\");\n    }\n\n    // Cleanup\n    CUDA_CHECK(cudaGraphExecDestroy(graphExec));\n    CUDA_CHECK(cudaGraphDestroy(graph));\n    CUDA_CHECK(cudaGraphExecDestroy(manualGraphExec));\n    CUDA_CHECK(cudaGraphDestroy(manualGraph));\n    CUDA_CHECK(cudaEventDestroy(start));\n    CUDA_CHECK(cudaEventDestroy(stop));\n    CUDA_CHECK(cudaStreamDestroy(stream));\n    CUDA_CHECK(cudaFree(d_data));\n    free(h_result);\n\n    printf(\"\\nNote: CUDA Graphs are most beneficial when:\\n\");\n    printf(\"  - Same sequence of operations repeated many times\\n\");\n    printf(\"  - CPU launch overhead is significant\\n\");\n    printf(\"  - Operations are small and numerous\\n\");\n\n    return 0;\n}\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise",
    "",
    "Complete the following exercises to practice the concepts learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n#include <stdio.h>\n#include <stdlib.h>\n#include <cuda_runtime.h>\n\n#define CUDA_CHECK(call) \\\n    do { \\\n        cudaError_t err = call; \\\n        if (err != cudaSuccess) { \\\n            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", \\\n                    __FILE__, __LINE__, cudaGetErrorString(err)); \\\n            exit(EXIT_FAILURE); \\\n        } \\\n    } while(0)\n\n__global__ void kernel(float *data, int n) {\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < n) {\n        data[idx] = data[idx] * 2.0f;\n    }\n}\n\nint main() {\n    printf(\"=== Cuda Graphs ===\\n\\n\");\n\n    int n = 1000000;\n    size_t size = n * sizeof(float);\n\n    float *h_data = (float*)malloc(size);\n    for (int i = 0; i < n; i++) h_data[i] = i;\n\n    float *d_data;\n    CUDA_CHECK(cudaMalloc(&d_data, size));\n    CUDA_CHECK(cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice));\n\n    cudaEvent_t start, stop;\n    cudaEventCreate(&start);\n    cudaEventCreate(&stop);\n\n    int threads = 256;\n    int blocks = (n + threads - 1) / threads;\n\n    cudaEventRecord(start);\n    kernel<<<blocks, threads>>>(d_data, n);\n    cudaEventRecord(stop);\n    cudaEventSynchronize(stop);\n\n    float ms;\n    cudaEventElapsedTime(&ms, start, stop);\n\n    CUDA_CHECK(cudaMemcpy(h_data, d_data, size, cudaMemcpyDeviceToHost));\n\n    printf(\"Processed %d elements in %.2f ms\\n\", n, ms);\n    printf(\"Bandwidth: %.2f GB/s\\n\", (size * 2 / 1e9) / (ms / 1000.0));\n\n    free(h_data);\n    cudaFree(d_data);\n    cudaEventDestroy(start);\n    cudaEventDestroy(stop);\n\n    return 0;\n}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n\n1. CUDA graphs reduce launch overhead\n2. Record operations once, replay many times\n3. Faster than individual launches\n4. Great for recurring workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps",
    "",
    "Continue to: **52_next_topic.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes",
    "",
    "*Use this space to write your own notes and observations:*",
    "",
    "---",
    "",
    "",
    "",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}