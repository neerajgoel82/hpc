{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Hello World from GPU\n",
    "## Phase 1: Foundations - CUDA Architecture Basics\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the basic CUDA programming model\n",
    "- Learn how to write and launch a simple CUDA kernel\n",
    "- Understand the difference between host (CPU) and device (GPU) code\n",
    "- Learn how to compile and execute CUDA programs\n",
    "- Understand kernel launch syntax `<<<grid, block>>>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept: CUDA Programming Model\n",
    "\n",
    "CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and programming model. It allows developers to use GPUs for general-purpose processing.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Host**: The CPU and its memory (RAM)\n",
    "- **Device**: The GPU and its memory (VRAM)\n",
    "- **Kernel**: A function that runs on the GPU\n",
    "- **Thread**: A single execution unit on the GPU\n",
    "\n",
    "**CUDA Program Structure:**\n",
    "1. Allocate memory on GPU\n",
    "2. Copy data from host to device\n",
    "3. Launch kernel on GPU\n",
    "4. Copy results back from device to host\n",
    "5. Free GPU memory\n",
    "\n",
    "**Kernel Launch Syntax:**\n",
    "```cuda\n",
    "kernelFunction<<<numBlocks, threadsPerBlock>>>(arguments);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Hello World Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "// Kernel definition - runs on GPU\n",
    "__global__ void helloFromGPU() {\n",
    "    printf(\"Hello World from GPU!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // Host code\n",
    "    printf(\"Hello World from CPU!\\n\");\n",
    "    \n",
    "    // Launch kernel with 1 block and 1 thread\n",
    "    helloFromGPU<<<1, 1>>>();\n",
    "    \n",
    "    // Wait for GPU to finish\n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multiple Threads Saying Hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void helloFromMultipleThreads() {\n",
    "    printf(\"Hello from thread %d!\\n\", threadIdx.x);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching kernel with 10 threads...\\n\");\n",
    "    \n",
    "    // Launch kernel with 1 block and 10 threads\n",
    "    helloFromMultipleThreads<<<1, 10>>>();\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Multiple Blocks and Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void helloFromBlocksAndThreads() {\n",
    "    printf(\"Hello from block %d, thread %d!\\n\", blockIdx.x, threadIdx.x);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching kernel with 3 blocks and 4 threads per block...\\n\");\n",
    "    \n",
    "    // Launch kernel with 3 blocks and 4 threads per block\n",
    "    helloFromBlocksAndThreads<<<3, 4>>>();\n",
    "    \n",
    "    cudaDeviceSynchronize();\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Basic Error Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "#include <stdio.h>\n",
    "\n",
    "// Macro for error checking\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            printf(\"CUDA error in %s:%d: %s\\n\", __FILE__, __LINE__, \\\n",
    "                   cudaGetErrorString(err)); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while(0)\n",
    "\n",
    "__global__ void simpleKernel() {\n",
    "    printf(\"Kernel executed successfully!\\n\");\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    printf(\"Launching kernel with error checking...\\n\");\n",
    "    \n",
    "    simpleKernel<<<1, 1>>>();\n",
    "    \n",
    "    // Check for kernel launch errors\n",
    "    CUDA_CHECK(cudaGetLastError());\n",
    "    \n",
    "    // Synchronize and check for execution errors\n",
    "    CUDA_CHECK(cudaDeviceSynchronize());\n",
    "    \n",
    "    printf(\"Kernel completed without errors!\\n\");\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise\n",
    "\n",
    "**Exercise 1:** Modify the hello world kernel to print a custom message with both block and thread IDs.\n",
    "\n",
    "**Exercise 2:** Launch a kernel with 5 blocks and 8 threads per block. How many total threads are executing?\n",
    "\n",
    "**Exercise 3:** Add error checking to ensure the kernel launches successfully.\n",
    "\n",
    "**Exercise 4:** Experiment with different block and thread configurations. What happens with very large numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cu\n",
    "// Your solution here\n",
    "#include <stdio.h>\n",
    "\n",
    "__global__ void myCustomKernel() {\n",
    "    // TODO: Implement your custom kernel\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    // TODO: Launch your kernel\n",
    "    \n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **CUDA kernels** are functions that run on the GPU, defined with `__global__` keyword\n",
    "2. **Kernel launch syntax**: `kernelName<<<numBlocks, threadsPerBlock>>>(args)`\n",
    "3. **threadIdx.x** gives the thread index within a block\n",
    "4. **blockIdx.x** gives the block index within the grid\n",
    "5. **cudaDeviceSynchronize()** waits for all GPU operations to complete\n",
    "6. Always check for CUDA errors in production code\n",
    "7. GPU threads execute in parallel, so output order may vary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In the next notebook, we'll learn how to:\n",
    "- Query GPU device properties\n",
    "- Understand GPU architecture (SMs, cores, warps)\n",
    "- Check GPU capabilities programmatically\n",
    "- Make informed decisions about kernel configuration\n",
    "\n",
    "Continue to: **02_device_query.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "*Use this space to write your own notes and observations:*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
