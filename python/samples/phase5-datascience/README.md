# Phase 5: Data Science with Python

**Duration**: 6-8 weeks
**Level**: Intermediate to Advanced
**Prerequisites**: Completed Phases 1-4

---

## Overview

Phase 5 introduces data science with Python, covering numerical computing, data manipulation, visualization, statistical analysis, and machine learning basics. You'll learn industry-standard tools and techniques used by data scientists worldwide.

---

## Learning Objectives

By the end of this phase, you will:
- ‚úÖ Master NumPy for numerical computing
- ‚úÖ Use Pandas for data manipulation and analysis
- ‚úÖ Create professional visualizations
- ‚úÖ Clean and preprocess real-world datasets
- ‚úÖ Perform statistical analysis
- ‚úÖ Build basic machine learning models
- ‚úÖ Complete end-to-end data science projects

---

## Module Structure

### 01. NumPy - Numerical Computing (Week 1)
**Directory**: `01-numpy/`

**Topics**:
- NumPy arrays and operations
- Array indexing and slicing
- Broadcasting and vectorization
- Linear algebra operations
- Random number generation
- Performance optimization

**Files**:
- `01_arrays_basics.py` - Creating and manipulating arrays
- `02_indexing_slicing.py` - Advanced indexing techniques
- `03_broadcasting.py` - Broadcasting rules and examples
- `04_linear_algebra.py` - Matrix operations, dot products
- `05_random_sampling.py` - Random number generation
- `06_performance.py` - Vectorization vs loops
- `exercises.py` - Practice problems

**Key Concepts**: Vectorization, broadcasting, dtype optimization

---

### 02. Pandas - Data Manipulation (Week 2)
**Directory**: `02-pandas/`

**Topics**:
- Series and DataFrames
- Reading/writing data (CSV, Excel, JSON)
- Data selection and filtering
- GroupBy operations
- Merging and joining datasets
- Time series analysis

**Files**:
- `01_series_dataframes.py` - Basic Pandas structures
- `02_reading_data.py` - Loading various file formats
- `03_selection_filtering.py` - Data querying
- `04_groupby_aggregation.py` - Group operations
- `05_merging_joining.py` - Combining datasets
- `06_time_series.py` - Working with dates and times
- `exercises.py` - Practice problems

**Key Concepts**: DataFrame operations, method chaining, tidy data

---

### 03. Data Visualization (Week 3)
**Directory**: `03-visualization/`

**Topics**:
- Matplotlib fundamentals
- Seaborn for statistical plots
- Customizing plots
- Subplots and layouts
- Interactive visualizations (Plotly basics)
- Best practices for data visualization

**Files**:
- `01_matplotlib_basics.py` - Line plots, scatter plots, bars
- `02_seaborn_statistical.py` - Distribution plots, box plots
- `03_customization.py` - Colors, styles, annotations
- `04_subplots_layouts.py` - Multiple plot arrangements
- `05_plotly_interactive.py` - Interactive charts
- `06_best_practices.py` - Effective visualization principles
- `exercises.py` - Create various charts

**Key Concepts**: Chart selection, color theory, storytelling with data

---

### 04. Data Cleaning & Preprocessing (Week 4)
**Directory**: `04-data-cleaning/`

**Topics**:
- Handling missing data
- Data type conversions
- Outlier detection and treatment
- Feature engineering
- Data normalization/standardization
- Text data preprocessing

**Files**:
- `01_missing_data.py` - Handling NaN values
- `02_data_types.py` - Type conversions and validation
- `03_outliers.py` - Detecting and handling outliers
- `04_feature_engineering.py` - Creating new features
- `05_scaling_normalization.py` - Scaling techniques
- `06_text_preprocessing.py` - Cleaning text data
- `exercises.py` - Clean messy datasets

**Key Concepts**: Data quality, feature engineering, preprocessing pipelines

---

### 05. Statistical Analysis (Week 5)
**Directory**: `05-statistical-analysis/`

**Topics**:
- Descriptive statistics
- Probability distributions
- Hypothesis testing
- Correlation analysis
- Chi-square tests
- ANOVA
- Statistical visualization

**Files**:
- `01_descriptive_stats.py` - Mean, median, variance, etc.
- `02_distributions.py` - Normal, binomial, Poisson
- `03_hypothesis_testing.py` - t-tests, z-tests
- `04_correlation.py` - Pearson, Spearman correlation
- `05_chi_square.py` - Categorical data analysis
- `06_anova.py` - Analysis of variance
- `exercises.py` - Statistical tests

**Key Concepts**: Statistical inference, p-values, confidence intervals

---

### 06. Machine Learning Basics (Week 6-7)
**Directory**: `06-ml-basics/`

**Topics**:
- Introduction to scikit-learn
- Train-test split and cross-validation
- Linear regression
- Logistic regression
- Decision trees
- Random forests
- K-Nearest Neighbors
- Model evaluation metrics

**Files**:
- `01_sklearn_intro.py` - Scikit-learn workflow
- `02_train_test_split.py` - Data splitting
- `03_linear_regression.py` - Regression modeling
- `04_logistic_regression.py` - Classification
- `05_decision_trees.py` - Tree-based models
- `06_random_forests.py` - Ensemble methods
- `07_knn.py` - K-Nearest Neighbors
- `08_model_evaluation.py` - Metrics and validation
- `exercises.py` - Build and evaluate models

**Key Concepts**: Supervised learning, overfitting, bias-variance tradeoff

---

### 07. Projects (Week 8)
**Directory**: `07-projects/`

**End-to-End Projects**:

1. **Sales Analysis Dashboard**
   - `project_sales_analysis.py`
   - Analyze sales data, trends, visualizations
   - Use: Pandas, Matplotlib, Seaborn

2. **Customer Segmentation**
   - `project_customer_segmentation.py`
   - Cluster customers using K-Means
   - Use: Pandas, scikit-learn, visualization

3. **Housing Price Prediction**
   - `project_housing_prices.py`
   - Predict house prices with regression
   - Use: Full ML pipeline

4. **Churn Prediction**
   - `project_churn_prediction.py`
   - Predict customer churn
   - Use: Classification, feature engineering

5. **Exploratory Data Analysis (EDA)**
   - `project_eda_template.py`
   - Complete EDA workflow
   - Use: All tools learned

---

## Prerequisites

### Knowledge
- Python fundamentals (Phases 1-4)
- Basic statistics (helpful but not required)
- Understanding of linear algebra (helpful)

### Tools Required
```bash
pip install numpy pandas matplotlib seaborn scikit-learn jupyter
pip install plotly scipy statsmodels
```

---

## Learning Path

### Week 1: NumPy Mastery
```
Day 1-2: Arrays and operations
Day 3-4: Indexing and broadcasting
Day 5-7: Linear algebra and optimization
```

### Week 2: Pandas Proficiency
```
Day 1-2: DataFrames basics
Day 3-4: Data manipulation
Day 5-7: GroupBy and merging
```

### Week 3: Visualization Skills
```
Day 1-3: Matplotlib and Seaborn
Day 4-5: Customization and layouts
Day 6-7: Interactive plots and best practices
```

### Week 4: Data Cleaning
```
Day 1-2: Missing data and types
Day 3-4: Outliers and features
Day 5-7: Complete preprocessing pipelines
```

### Week 5: Statistical Analysis
```
Day 1-3: Descriptive and inferential stats
Day 4-5: Hypothesis testing
Day 6-7: Advanced statistical techniques
```

### Week 6-7: Machine Learning
```
Week 6: Regression and classification basics
Week 7: Advanced models and evaluation
```

### Week 8: Projects
```
Complete 2-3 end-to-end projects
Build portfolio pieces
```

---

## Installation & Setup

### Create Virtual Environment
```bash
cd python/samples/phase5-datascience
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Install Dependencies
```bash
pip install -r requirements.txt
```

### Verify Installation
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets

print("NumPy version:", np.__version__)
print("Pandas version:", pd.__version__)
print("All libraries loaded successfully!")
```

---

## Datasets

Sample datasets included in `datasets/`:
- `sales_data.csv` - Retail sales data
- `customer_data.csv` - Customer information
- `housing_data.csv` - Housing prices
- `churn_data.csv` - Customer churn data
- `weather_data.csv` - Weather observations

**External datasets** (download separately):
- Kaggle datasets
- UCI Machine Learning Repository
- scikit-learn built-in datasets

---

## Best Practices

### Data Science Workflow
1. **Define the problem** - What question are you answering?
2. **Collect data** - Gather relevant data
3. **Clean data** - Handle missing values, outliers
4. **Explore data** - EDA, visualizations
5. **Feature engineering** - Create useful features
6. **Model building** - Train ML models
7. **Evaluation** - Assess model performance
8. **Deploy/Report** - Share insights or deploy model

### Code Organization
```python
# Standard structure
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load data
df = pd.read_csv('data.csv')

# Explore data
print(df.info())
print(df.describe())

# Clean data
df = df.dropna()

# Analyze/visualize
# ... analysis code ...

# Save results
df.to_csv('results.csv', index=False)
```

### Performance Tips
- Use vectorized operations (avoid loops)
- Use appropriate data types (category, int32 vs int64)
- Process data in chunks for large datasets
- Profile code with `%%timeit` in Jupyter

---

## Common Patterns

### Reading and Exploring Data
```python
import pandas as pd

# Read data
df = pd.read_csv('data.csv')

# Quick exploration
print(df.head())
print(df.info())
print(df.describe())
print(df.isnull().sum())
```

### Data Cleaning Pipeline
```python
def clean_data(df):
    # Handle missing values
    df = df.dropna(subset=['important_column'])
    df['other_column'] = df['other_column'].fillna(df['other_column'].mean())
    
    # Remove duplicates
    df = df.drop_duplicates()
    
    # Fix data types
    df['date'] = pd.to_datetime(df['date'])
    df['category'] = df['category'].astype('category')
    
    return df

df_clean = clean_data(df)
```

### Visualization Template
```python
import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style('whitegrid')

# Create figure
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Plot 1
axes[0, 0].plot(x, y)
axes[0, 0].set_title('Title')

# ... more plots ...

plt.tight_layout()
plt.show()
```

---

## Resources

### Books
- "Python for Data Analysis" by Wes McKinney
- "Hands-On Machine Learning" by Aur√©lien G√©ron
- "Introduction to Statistical Learning" (Free PDF)

### Online Courses
- Kaggle Learn (Free micro-courses)
- DataCamp
- Coursera: Applied Data Science with Python

### Documentation
- [NumPy Documentation](https://numpy.org/doc/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib Documentation](https://matplotlib.org/)
- [Scikit-learn Documentation](https://scikit-learn.org/)

### Practice Platforms
- Kaggle Competitions
- DataCamp Projects
- Real Python Tutorials

---

## Next Steps

After completing Phase 5:
- **Phase 6**: PyTorch for Deep Learning
- **Advanced Data Science**: Time series, NLP, advanced ML
- **Kaggle Competitions**: Apply skills competitively
- **Portfolio Projects**: Build showcase projects
- **Certifications**: Consider data science certifications

---

## Tips for Success

1. **Code along**: Don't just read, write the code
2. **Work with real data**: Use Kaggle datasets
3. **Visualize everything**: Plots reveal insights
4. **Document your work**: Use Jupyter notebooks
5. **Join communities**: Kaggle, Reddit r/datascience
6. **Build projects**: Apply skills to real problems
7. **Read others' code**: Learn from Kaggle kernels
8. **Stay updated**: Data science evolves rapidly

---

**Ready to start?** Begin with `01-numpy/01_arrays_basics.py` and work through each module sequentially! üìäüêç
